{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bacaf27",
   "metadata": {},
   "source": [
    "# Location Finding Example (BED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac719090",
   "metadata": {},
   "source": [
    "This notebook demonstrates:\n",
    "\n",
    "- How to train deterministic and stochastic policies using the Barber–Agakov lower bound, jointly with a CouplingFlow posterior.  \n",
    "- The simulator, neural networks, and training loop, along with experiments and plotting code to compare policies under prior and simulator shift.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88acee62",
   "metadata": {},
   "source": [
    "### 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83467b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main dependencies:\n",
    "import os\n",
    "if \"KERAS_BACKEND\" not in os.environ:\n",
    "  os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.distributions as dist\n",
    "from torch.distributions import transforms\n",
    "from torch import Size, Tensor\n",
    "from bayesflow.networks import CouplingFlow, FlowMatching\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# For plotting:\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from scipy.stats import gaussian_kde\n",
    "from scipy.interpolate import RBFInterpolator\n",
    "from matplotlib.ticker import FormatStrFormatter\n",
    "from matplotlib.ticker import MaxNLocator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d6a77c",
   "metadata": {},
   "source": [
    "### 2. Simulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9340ba4b",
   "metadata": {},
   "source": [
    "- This is the main class used to sample $\\theta$ from the prior and to roll out trajectories under a given policy. \n",
    "- Each `LocationFinding` instance is associated with its own policy instance. \n",
    "- Note that we define separately the bounds of the prior (from which $\\theta$ is sampled) and the bounds within which the policy can place designs, as this will be more convenient when analyzing prior shift.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9503af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocationFinding(nn.Module):\n",
    "    def __init__(self,\n",
    "                 policy: nn.Module,\n",
    "                 prior_bounds: float = 3.0,           # Side length of uniform prior        \n",
    "                 policy_bounds: float = 3.0,          # Side length of the design space\n",
    "                 D: int = 1,                          # Number of sensors each time period\n",
    "                 K: int = 1,                          # Number of sources\n",
    "                 a: int | list = 1,                   # Signal weights for each source\n",
    "                 m: float = 0.001,                    # Minimum squared distance to a source\n",
    "                 b: float = 0.1,                      # Background signal\n",
    "                 noise_std: float = 0.5,              # Noise added to the aggregate signal\n",
    "                 T: int = 7,                          # Number of time periods\n",
    "                 p: int = 2                           # Number of coordinate dimensions\n",
    "    ) -> None:\n",
    "        super(LocationFinding, self).__init__()\n",
    "\n",
    "        self.policy = policy\n",
    "        self.policy_bounds= policy_bounds\n",
    "        self.D = D\n",
    "        self.K = K\n",
    "        self.m = m\n",
    "        self.b = b\n",
    "        self.noise_std = noise_std\n",
    "        self.T = T\n",
    "        self.p = p\n",
    "\n",
    "        dtype = torch.float32\n",
    "        if not isinstance(a, torch.Tensor):\n",
    "            a = torch.tensor(a, dtype=dtype)\n",
    "        self.register_buffer('a', a)\n",
    "        self.register_buffer('_cov_matrix', noise_std**2 * torch.eye(D, dtype=dtype))\n",
    "        self.register_buffer('_prior_low', -prior_bounds * torch.ones(K * p, dtype=dtype))\n",
    "        self.register_buffer('_prior_high', prior_bounds * torch.ones(K * p, dtype=dtype))\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return self.a.device\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.a.dtype\n",
    "\n",
    "    def prior(self):\n",
    "        base = dist.Uniform(self._prior_low, self._prior_high)  # [K*p]\n",
    "        return dist.Independent(base, 1)                        # Event shape: [K*p]\n",
    "\n",
    "    def outcome_likelihood(self, theta: Tensor, designs: Tensor):\n",
    "        batch_shape = theta.shape[:-1]\n",
    "        theta = theta.view(*batch_shape, self.K, self.p)        # [*B, K, p]\n",
    "\n",
    "        # Pair-wise distances between sensors and sources for each batch\n",
    "        distances = torch.cdist(designs, theta)                 # [*B, D, K], since torch.cdist(BxDxp, BxKxp) -> BxDxK\n",
    "        signals = self.a / (self.m + distances**2)              # [*B, D, K]\n",
    "        total_signal = signals.sum(dim=-1) + self.b             # [*B, D]\n",
    "        loc = torch.log(total_signal)\n",
    "\n",
    "        noisy_signal = dist.MultivariateNormal(loc=loc, covariance_matrix=self._cov_matrix)\n",
    "\n",
    "        return noisy_signal\n",
    "\n",
    "    def forward(self, theta: Tensor, entropy_bonus: bool = False):\n",
    "        \"\"\"Simulates trajectories under a given batch of thetas.\"\"\"\n",
    "        batch_shape = theta.shape[:-1]\n",
    "        designs  = torch.zeros(*batch_shape, self.T, self.D, self.p, device=theta.device, dtype=theta.dtype)    # [*B, T, D, p]\n",
    "        outcomes = torch.zeros(*batch_shape, self.T, self.D, device=theta.device, dtype=theta.dtype)            # [*B, T, D]\n",
    "        entropies = None\n",
    "\n",
    "        if entropy_bonus:\n",
    "            entropies = torch.empty(*batch_shape, self.T, device=theta.device, dtype=theta.dtype)               # [*B, T]\n",
    "\n",
    "        # Generate trajectories under the current policy\n",
    "        for t in range(self.T):\n",
    "            hist_designs = designs[..., :t, :, :]                # [*B, t, D, p]\n",
    "            hist_outcomes = outcomes[..., :t, :]                 # [*B, t, D]\n",
    "\n",
    "            xi_t = self.policy(hist_designs, hist_outcomes)\n",
    "            y_t = self.outcome_likelihood(theta, xi_t).rsample()\n",
    "\n",
    "            if entropy_bonus:\n",
    "                entropies[..., t] = self.policy._entropy(hist_designs, hist_outcomes)        # [*B]\n",
    "\n",
    "            designs[..., t, :, :] = xi_t\n",
    "            outcomes[..., t, :] = y_t\n",
    "\n",
    "        return designs, outcomes, entropies\n",
    "\n",
    "    def sample(self, batch_shape: Size, entropy_bonus: bool = False):\n",
    "        \"\"\"Randomly sample theta from prior and generate trajectories.\"\"\"\n",
    "        theta = self.prior().sample(batch_shape)      # [*B, K*p]\n",
    "        designs, outcomes, entropies = self(theta, entropy_bonus=entropy_bonus)\n",
    "\n",
    "        return {\n",
    "            \"theta\": theta,             # [*B, K*p]\n",
    "            \"designs\": designs,         # [*B, T, D, p]\n",
    "            \"outcomes\": outcomes,       # [*B, T, D]\n",
    "            \"entropies\": entropies}     # [*B, T] or None\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def run_policy(self, theta: Tensor, entropy_bonus: bool = False):\n",
    "        \"\"\"forward() call without gradients and in evaluation mode.\"\"\"\n",
    "        self.policy.eval()\n",
    "        designs, outcomes, entropies = self(theta, entropy_bonus=entropy_bonus)\n",
    "        self.policy.train()\n",
    "\n",
    "        return designs, outcomes, entropies\n",
    "\n",
    "    def plot_designs(self,\n",
    "                    theta: Tensor,\n",
    "                    designs: Tensor) -> None:\n",
    "        \"\"\"Plot designs from the policy network under a given set of thetas\"\"\"\n",
    "        assert self.p == 2, \"only 2-D plotting supported\"\n",
    "\n",
    "        theta = theta.cpu()\n",
    "        designs = designs.cpu()\n",
    "        num_periods = designs.shape[1]            # T\n",
    "        B = theta.size(0)                         # Only plot the first batch dimension\n",
    "        theta = theta.view(B, self.K, self.p)     # [*B, K, p]\n",
    "\n",
    "        # Initialize the signal field \n",
    "        grid_size = 100\n",
    "        x_vals = torch.linspace(-self.policy_bounds, self.policy_bounds, grid_size)\n",
    "        y_vals = torch.linspace(-self.policy_bounds, self.policy_bounds, grid_size)\n",
    "        X, Y   = torch.meshgrid(x_vals, y_vals, indexing=\"ij\")\n",
    "        grid_pts = torch.cartesian_prod(x_vals, y_vals)     # [G, 2] := [grid_size*grid_size, 2]\n",
    "\n",
    "        # Initialize subplots\n",
    "        max_cols = 4\n",
    "        ncols = min(B, max_cols)\n",
    "        nrows = math.ceil(B / ncols)\n",
    "\n",
    "        fig, axes = plt.subplots(\n",
    "            nrows, ncols,\n",
    "            figsize=(4.5*ncols, 4*nrows),\n",
    "            sharex=True,\n",
    "            sharey=True,\n",
    "            squeeze=False,\n",
    "            dpi=300\n",
    "        )\n",
    "        flat_axes = axes.flatten()\n",
    "\n",
    "        red = np.array(mcolors.to_rgb(\"#b22222\"))\n",
    "        yellow = np.array(mcolors.to_rgb(\"#ffeb4d\"))\n",
    "\n",
    "        # Loop over each realization\n",
    "        for b in range(B):\n",
    "            ax = flat_axes[b]\n",
    "\n",
    "            dist_field = torch.cdist(\n",
    "                grid_pts.unsqueeze(0),              # [1, G, 2]\n",
    "                theta[b].unsqueeze(0)               # [1, K, 2]\n",
    "            )                                       # [1, G, K]\n",
    "\n",
    "            a_cpu = self.a.cpu()\n",
    "            field = (a_cpu / (self.m + dist_field.pow(2))).sum(dim=-1) + self.b    # [1, G]\n",
    "            Z = torch.log(field.view(grid_size, grid_size)).detach().numpy()\n",
    "        \n",
    "            blues = plt.colormaps['Blues']\n",
    "            colors = blues(np.linspace(0, 1, 256))\n",
    "            colors[:, :3] = colors[:, :3] ** 1.25\n",
    "            dark_blues = LinearSegmentedColormap.from_list('dark_blues', colors)\n",
    "        \n",
    "            im = ax.contourf(\n",
    "                X.detach().numpy(), Y.detach().numpy(), Z,\n",
    "                levels=30, cmap=dark_blues, alpha=1.0,\n",
    "                vmin=np.min(Z),\n",
    "                vmax=np.max(Z)\n",
    "            )\n",
    "\n",
    "            ## Plot the design trajectory\n",
    "            for t in range(num_periods):\n",
    "                xi_t = designs[b, t]            # [D, 2]\n",
    "                frac = (t + 1) / num_periods\n",
    "                color = red * (1 - frac) + yellow * frac\n",
    "                ax.scatter(\n",
    "                    xi_t[:, 0].detach().numpy(), xi_t[:, 1].detach().numpy(),\n",
    "                    c=[color], edgecolors=\"k\", s=70\n",
    "                )\n",
    "\n",
    "            ax.set_aspect(\"equal\")\n",
    "            ax.set_xlim(-self.policy_bounds, self.policy_bounds)\n",
    "            ax.set_ylim(-self.policy_bounds, self.policy_bounds)\n",
    "            ax.grid(True, color=\"lightgray\", linewidth=0.3, alpha=0.3)\n",
    "            ax.set_title(f\"Trajectory {b+1}\")\n",
    "\n",
    "        for ax in flat_axes[B:]:\n",
    "            ax.set_visible(False)\n",
    "\n",
    "        fig.tight_layout()\n",
    "\n",
    "        # Colorbar 1: Signal field \n",
    "        signal_cbar = fig.colorbar(\n",
    "            im,\n",
    "            ax=flat_axes[:B],\n",
    "            location=\"right\",\n",
    "            label=\"Log Total Signal\",\n",
    "            format=\"%.2f\",\n",
    "            pad=-0.09,\n",
    "            shrink=0.9,\n",
    "        )\n",
    "        vmin, vmax = im.get_clim()\n",
    "        signal_cbar.set_ticks(np.linspace(vmin, vmax, 6))\n",
    "\n",
    "        # Colorbar 2: Time-step \n",
    "        time_cmap = LinearSegmentedColormap.from_list('red_yellow', [red, yellow], N=num_periods)\n",
    "        time_sm = cm.ScalarMappable(cmap=time_cmap, norm=plt.Normalize(vmin=1, vmax=num_periods))\n",
    "        time_sm.set_array([])\n",
    "        time_cbar = fig.colorbar(\n",
    "            time_sm,\n",
    "            ax=flat_axes[:B],\n",
    "            location=\"right\",\n",
    "            label=\"Timestep\",\n",
    "            format=\"%d\",\n",
    "            pad=0.025,\n",
    "            shrink=0.9,\n",
    "        )\n",
    "        time_cbar.set_ticks(np.arange(1, num_periods + 1))\n",
    "\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0d7866",
   "metadata": {},
   "source": [
    "### 3. Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c531bd5",
   "metadata": {},
   "source": [
    "- Defines the main deterministic and stochastic policy networks, as well as the history encoder module.  \n",
    "- Defines a separate random policy that places uniformly random designs within the policy bounds. This network has no trainable parameters but is implemented as an `nn.Module` for compatibility with the `LocationFinding` class.  \n",
    "- Defines a posterior network, which serves as a convenient wrapper around BayesFlow amortized posteriors. Either `CouplingFlow` or `FlowMatching` can be used as the base posterior, but `CouplingFlow` is recommended because it is significantly faster while still reasonably flexible.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00d94f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryEncoder(nn.Module):\n",
    "    def __init__(self,\n",
    "                 D: int,\n",
    "                 p: int,\n",
    "                 enc_h_dim: int = 256,\n",
    "                 enc_out_dim: int = 128):\n",
    "        super().__init__()\n",
    "        self.enc_out_dim = enc_out_dim\n",
    "        self.enc_fc1 = nn.Linear(D*p + D, enc_h_dim)\n",
    "        self.enc_fc2 = nn.Linear(enc_h_dim, enc_out_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, hist_designs, hist_outcomes):\n",
    "        batch_shape = hist_designs.shape[:-3]\n",
    "        T = hist_designs.shape[-3]\n",
    "        device = hist_designs.device\n",
    "        dtype = hist_designs.dtype\n",
    "\n",
    "        if T == 0:\n",
    "            return torch.zeros(*batch_shape, self.enc_out_dim, device=device, dtype=dtype)\n",
    "\n",
    "        xi_flat = hist_designs.flatten(start_dim=-2, end_dim=-1)    # [*B, T, D*p]\n",
    "        inp = torch.cat([xi_flat, hist_outcomes], dim=-1)           # [*B, T, D*p + D]\n",
    "\n",
    "        h = self.relu(self.enc_fc1(inp))    # [*B, T, enc_h_dim]\n",
    "        out = self.enc_fc2(h)               # [*B, T, enc_out_dim]\n",
    "        rep = out.mean(dim=-2)              # [*B, enc_out_dim]\n",
    "\n",
    "        return rep\n",
    "\n",
    "class DeterministicPolicyNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 D: int,\n",
    "                 p: int,\n",
    "                 policy_bounds: float,\n",
    "                 enc_h_dim: int = 256,\n",
    "                 enc_out_dim: int = 128,\n",
    "                 h_dim: int = 64):\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "        self.p = p\n",
    "        self.policy_bounds = policy_bounds\n",
    "\n",
    "        self.history_encoder = HistoryEncoder(D=D, p=p, enc_h_dim=enc_h_dim, enc_out_dim=enc_out_dim)\n",
    "\n",
    "        self.fc1 = nn.Linear(enc_out_dim, h_dim)\n",
    "        self.fc2 = nn.Linear(h_dim, D*p)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Initialization\n",
    "        nn.init.xavier_uniform_(self.fc1.weight, gain=nn.init.calculate_gain(\"relu\"))\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight, gain=1.0)\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def forward(self, hist_designs, hist_outcomes):\n",
    "        batch_shape = hist_designs.shape[:-3]\n",
    "\n",
    "        rep = self.history_encoder(hist_designs, hist_outcomes)\n",
    "\n",
    "        h = self.relu(self.fc1(rep))     # [*B, D*p]\n",
    "        out = self.fc2(h)\n",
    "        out = out.view(*batch_shape, self.D, self.p)\n",
    "\n",
    "        return torch.tanh(out) * self.policy_bounds\n",
    "\n",
    "class StochasticPolicyNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                 D: int,\n",
    "                 p: int,\n",
    "                 policy_bounds: float,\n",
    "                 enc_h_dim: int = 256,\n",
    "                 enc_out_dim: int = 128,\n",
    "                 h_dim: int = 64,\n",
    "                 min_std: float = 0.01,\n",
    "                 init_mean: float = 0.0,\n",
    "                 init_std: float = 0.5):\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "        self.p = p\n",
    "        self.policy_bounds = policy_bounds\n",
    "        self.min_std = min_std\n",
    "\n",
    "        self.history_encoder = HistoryEncoder(D=D, p=p, enc_h_dim=enc_h_dim, enc_out_dim=enc_out_dim)\n",
    "\n",
    "        self.fc1_mean = nn.Linear(enc_out_dim, h_dim)\n",
    "        self.fc2_mean = nn.Linear(h_dim, D*p)\n",
    "        self.fc1_raw_std = nn.Linear(enc_out_dim, h_dim)\n",
    "        self.fc2_raw_std = nn.Linear(h_dim, D*p)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.tanh_transform = transforms.TanhTransform(cache_size=1)\n",
    "        self.scale_transform = transforms.AffineTransform(loc=0, scale=policy_bounds)\n",
    "\n",
    "        # Initialization\n",
    "        nn.init.xavier_uniform_(self.fc1_mean.weight, gain=nn.init.calculate_gain(\"relu\"))\n",
    "        nn.init.zeros_(self.fc1_mean.bias)\n",
    "        nn.init.xavier_uniform_(self.fc2_mean.weight, gain=1.0)\n",
    "        nn.init.zeros_(self.fc2_mean.bias)\n",
    "        self.fc2_mean.bias.data.add_(init_mean)\n",
    "        nn.init.xavier_uniform_(self.fc1_raw_std.weight, gain=nn.init.calculate_gain(\"relu\"))\n",
    "        nn.init.zeros_(self.fc1_raw_std.bias)\n",
    "        nn.init.xavier_uniform_(self.fc2_raw_std.weight, gain=1.0)\n",
    "        nn.init.zeros_(self.fc2_raw_std.bias)\n",
    "        self.fc2_raw_std.bias.data.add_(math.log(max(init_std, 1e-8)))\n",
    "\n",
    "    def _base_distribution(self, hist_designs, hist_outcomes):\n",
    "        batch_shape = hist_designs.shape[:-3]\n",
    "\n",
    "        rep = self.history_encoder(hist_designs, hist_outcomes)    # [*B, enc_out_dim]\n",
    "\n",
    "        h_mean = self.relu(self.fc1_mean(rep))                     # [*B, h_dim]\n",
    "        out_mean = self.fc2_mean(h_mean)                           # [*B, D*p]\n",
    "        mean = out_mean.view((*batch_shape, self.D, self.p))       # [*B, D, p]\n",
    "\n",
    "        h_raw_std = self.relu(self.fc1_raw_std(rep))\n",
    "        out_raw_std = self.fc2_raw_std(h_raw_std)\n",
    "        raw_std = out_raw_std.view((*batch_shape, self.D, self.p))\n",
    "        std = torch.exp(raw_std) + self.min_std\n",
    "\n",
    "        return dist.Independent(dist.Normal(mean, std), reinterpreted_batch_ndims=2)    # Event shape: [D, p]\n",
    "\n",
    "    def _distribution(self, hist_designs, hist_outcomes):\n",
    "        base_dist = self._base_distribution(hist_designs, hist_outcomes)\n",
    "        return dist.TransformedDistribution(base_dist, [self.tanh_transform, self.scale_transform])\n",
    "\n",
    "    def _entropy(self, hist_designs, hist_outcomes, n_samples=1000):\n",
    "        base_dist = self._base_distribution(hist_designs, hist_outcomes)\n",
    "        base_entropy = base_dist.entropy()                    # [*B]\n",
    "        x = base_dist.rsample((n_samples,))                   # [n_samples, *B, D, p]\n",
    "\n",
    "        y_tanh = self.tanh_transform(x)\n",
    "        log_det_tanh = self.tanh_transform.log_abs_det_jacobian(x, y_tanh)\n",
    "        y_final = self.scale_transform(y_tanh)\n",
    "        log_det_scale = self.scale_transform.log_abs_det_jacobian(y_tanh, y_final)\n",
    "\n",
    "        total_log_det = log_det_tanh + log_det_scale          # [n_samples, *B, D, p]\n",
    "        total_log_det = total_log_det.sum(dim=[-2, -1])       # [n_samples, *B]\n",
    "\n",
    "        # Total entropy: H(Y) = H(X) + E[log|det J|]\n",
    "        return base_entropy + total_log_det.mean(dim=0)\n",
    "\n",
    "    def forward(self, hist_designs, hist_outcomes):\n",
    "        return self._distribution(hist_designs, hist_outcomes).rsample()     # [*B, D, p]\n",
    "    \n",
    "class RandomPolicyNet(nn.Module):\n",
    "    def __init__(self,\n",
    "                D: int,\n",
    "                p: int,\n",
    "                policy_bounds: float,\n",
    "                ):\n",
    "        super().__init__()\n",
    "        self.D = D\n",
    "        self.p = p\n",
    "        self.policy_bounds = policy_bounds\n",
    "\n",
    "    def forward(self, hist_designs, hist_outcomes):\n",
    "            batch_shape = hist_designs.shape[:-3]\n",
    "            device = hist_designs.device\n",
    "            dtype = hist_designs.dtype\n",
    "\n",
    "            rand_design = torch.rand(*batch_shape, self.D, self.p, device=device, dtype=dtype)\n",
    "            rand_design = rand_design.mul(2 * self.policy_bounds).sub(self.policy_bounds)\n",
    "\n",
    "            return rand_design\n",
    "    \n",
    "class PosteriorNet(nn.Module):\n",
    "      def __init__(self,\n",
    "                   D: int,\n",
    "                   p: int,\n",
    "                   K: int,\n",
    "                   enc_h_dim: int = 256,\n",
    "                   enc_out_dim: int = 128,\n",
    "                   inf_net: str = \"CouplingFlow\"   # or \"FlowMatching\"\n",
    "                   ):\n",
    "          super().__init__()\n",
    "          self.history_encoder = HistoryEncoder(D=D, p=p, enc_h_dim=enc_h_dim, enc_out_dim=enc_out_dim)\n",
    "\n",
    "          if inf_net == \"CouplingFlow\":\n",
    "              self.flow = CouplingFlow(\n",
    "                  subnet=\"mlp\",\n",
    "                  depth=6,\n",
    "                  transform=\"affine\",\n",
    "                  permutation=\"random\",\n",
    "                  use_actnorm=True,\n",
    "                  base_distribution=\"normal\",\n",
    "                  )\n",
    "\n",
    "          elif inf_net == \"FlowMatching\":\n",
    "              self.flow = FlowMatching(\n",
    "                  subnet=\"mlp\",\n",
    "                  subnet_kwargs={\"widths\": (256,)*6},      \n",
    "                  base_distribution=\"normal\",\n",
    "                  use_optimal_transport=True,\n",
    "                  loss_fn=\"mse\"\n",
    "                  )\n",
    "\n",
    "          self.flow.build(xz_shape=(32, K*p), conditions_shape=(32, enc_out_dim))\n",
    "\n",
    "      def forward(self, theta, designs, outcomes):\n",
    "          enc = self.history_encoder(designs, outcomes)\n",
    "          return self.flow.log_prob(theta, conditions=enc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c03bf2",
   "metadata": {},
   "source": [
    "### 4. Training Helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016d81cf",
   "metadata": {},
   "source": [
    "- Defines two functions that are useful for creating the joint policy–posterior training loop.  \n",
    "  - `train_policy()`: updates only the policy for a given posterior.  \n",
    "  - `warmup_posterior()`: updates only the posterior, and can be used to optionally warm up the posterior on purely random designs.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7f7cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_policy(simulator, \n",
    "                 posterior,\n",
    "                 optimizer,\n",
    "                 scheduler,\n",
    "                 num_steps: int = 3000,\n",
    "                 batch_size: int = 256,\n",
    "                 clip_norm: float | None = 2.0,\n",
    "                 entropy_bonus: bool = False,\n",
    "                 alpha: float = 0,\n",
    "                 alpha_decay: float = 1.0,\n",
    "                 verbose: bool = True):\n",
    "    \n",
    "    metrics = {\"ba_loss\": [], \"grad_norm\": [], \"mean_entropy\":[], \"alpha_values\": [], \"learning_rate\": []}\n",
    "    current_alpha = alpha\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        theta, designs, outcomes, entropies = simulator.sample(batch_shape=(batch_size,), entropy_bonus=entropy_bonus).values()\n",
    "\n",
    "        log_prob = posterior(theta, designs, outcomes)        # [*B]\n",
    "        loss = -log_prob.mean()\n",
    "\n",
    "        if entropies is not None and entropies.numel() > 0:\n",
    "            entropy_sum = entropies.sum(dim=-1)               # [*B]\n",
    "            mean_entropy = entropy_sum.mean()\n",
    "        else:\n",
    "            mean_entropy = torch.tensor(0.0, device=loss.device)\n",
    "\n",
    "        total_loss = loss - current_alpha*mean_entropy\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "\n",
    "        if clip_norm is not None:\n",
    "            torch.nn.utils.clip_grad_norm_(simulator.policy.parameters(), clip_norm)\n",
    "\n",
    "        grad_norm = torch.sqrt(\n",
    "            sum((p.grad.detach().pow(2).sum() \n",
    "                for p in simulator.policy.parameters() \n",
    "                if p.grad is not None), \n",
    "                torch.tensor(0.0)))\n",
    "\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        current_alpha *= alpha_decay\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "\n",
    "        # Store training metrics\n",
    "        metrics[\"ba_loss\"].append(loss.item())\n",
    "        metrics[\"grad_norm\"].append(grad_norm.item() if isinstance(grad_norm, torch.Tensor) else grad_norm)\n",
    "        metrics[\"mean_entropy\"].append(mean_entropy.item() if isinstance(mean_entropy, torch.Tensor) else mean_entropy)\n",
    "        metrics[\"alpha_values\"].append(current_alpha)\n",
    "        metrics[\"learning_rate\"].append(current_lr)\n",
    "\n",
    "        if verbose and ((step + 1) % 50 == 0 or step + 1 == 1):\n",
    "            if entropy_bonus:\n",
    "                print(f\"Step {step + 1}: BA loss {metrics['ba_loss'][-1]:.3f}   grad_norm {metrics['grad_norm'][-1]:.3f}  Entropy {metrics['mean_entropy'][-1]:.3f}  alpha {current_alpha:.4f}  lr {current_lr:.2e}\")\n",
    "            else:\n",
    "                print(f\"Step {step + 1}: BA loss {metrics['ba_loss'][-1]:.3f}   grad_norm {metrics['grad_norm'][-1]:.3f}  lr {current_lr:.2e}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "def warmup_posterior(simulator_rand,\n",
    "                     posterior,\n",
    "                     num_warmup_steps: int = 1000,\n",
    "                     batch_size: int = 256,\n",
    "                     verbose: bool = True):\n",
    "    with torch.enable_grad():  \n",
    "        optimizer = torch.optim.AdamW(posterior.parameters(), lr=1e-4)\n",
    "\n",
    "        for step in range(num_warmup_steps):\n",
    "            theta, designs, outcomes, _ = simulator_rand.sample(batch_shape=(batch_size,)).values()\n",
    "\n",
    "            log_prob = posterior(theta, designs, outcomes)\n",
    "            loss = -log_prob.mean()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if verbose and ((step + 1) % 50 == 0 or step + 1 == 1):\n",
    "                print(f\"Posterior warmup step {step + 1}: Loss = {loss.item():.3f}\")\n",
    "    \n",
    "    return posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7781b9fa",
   "metadata": {},
   "source": [
    "### 5. Joint Training Loop (Amortized Policy + Posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5add1a0",
   "metadata": {},
   "source": [
    "- Uses the helper functions to jointly train a policy and a posterior network using the Barber–Agakov lower bound.  \n",
    "- Optionally accepts a pre-trained (frozen) posterior. If provided, joint updates and warm-up are skipped, and only the policy is updated each iteration.  \n",
    "- Separately defines `posterior_bounds`, which specifies the area over which the posterior is warmed up.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1b54b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_joint(sim_params,\n",
    "                train_params,\n",
    "                policy_type=\"det\",       # \"det\", \"sto\", or \"rand\"\n",
    "                posterior=None):         # Optional (frozen) posterior overrides joint training and warmup\n",
    "\n",
    "    # Unwrap simulation params\n",
    "    (T, D, K, p, a, m, b, noise_std, prior_bounds, policy_bounds) = [sim_params[k] for k in [\n",
    "        \"T\", \"D\", \"K\", \"p\", \"a\", \"m\", \"b\",\n",
    "        \"noise_std\", \"prior_bounds\", \"policy_bounds\"]]\n",
    "\n",
    "    # Unwrap training params\n",
    "    (posterior_bounds, num_steps, num_warmup_steps, batch_size, clip_norm,\n",
    "    entropy_bonus, alpha, alpha_decay, device, verbose, plot_training) = [train_params[k] for k in [\n",
    "        \"posterior_bounds\", \"num_steps\", \"num_warmup_steps\",\n",
    "        \"batch_size\", \"clip_norm\", \"entropy_bonus\", \"alpha\",\n",
    "        \"alpha_decay\", \"device\", \"verbose\", \"plot_training\"]]\n",
    "\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if policy_type == \"det\":\n",
    "        policy = DeterministicPolicyNet(D=D, p=p, policy_bounds=policy_bounds)\n",
    "    elif policy_type == \"rand\":\n",
    "        policy = RandomPolicyNet(D=D, p=p, policy_bounds=policy_bounds)\n",
    "    elif policy_type == \"sto\":\n",
    "        policy = StochasticPolicyNet(D=D, p=p, policy_bounds=policy_bounds)\n",
    "\n",
    "    simulator = LocationFinding(\n",
    "        policy=policy,\n",
    "        prior_bounds=prior_bounds, \n",
    "        policy_bounds=policy_bounds, \n",
    "        D=D, K=K, a=a, m=m, b=b, noise_std=noise_std, T=T, p=p).to(device)\n",
    "    \n",
    "    if posterior is None:\n",
    "        simulator_warmup = LocationFinding(\n",
    "            policy=RandomPolicyNet(D=D, p=p, policy_bounds=posterior_bounds),\n",
    "            policy_bounds=posterior_bounds,\n",
    "            prior_bounds=posterior_bounds,\n",
    "            D=D, K=K, a=a, m=m, b=b, noise_std=noise_std, T=T, p=p).to(device)\n",
    "    \n",
    "        with torch.enable_grad():\n",
    "            posterior = PosteriorNet(D=D, p=p, K=K, inf_net=\"CouplingFlow\").to(device)\n",
    "            posterior = warmup_posterior(simulator_warmup, posterior, \n",
    "                                           num_warmup_steps=num_warmup_steps,\n",
    "                                           batch_size=batch_size, \n",
    "                                           verbose=verbose)\n",
    "    else:  \n",
    "        posterior_new = PosteriorNet(D=D, p=p, K=K, inf_net=\"CouplingFlow\").to(device)\n",
    "        posterior_new.load_state_dict(posterior.state_dict())\n",
    "        posterior = posterior_new\n",
    "    \n",
    "    # Training: for random policy, only train posterior; for others, train both policy and posterior\n",
    "    with torch.enable_grad():\n",
    "        if policy_type == \"rand\":\n",
    "            optimizer_params = list(posterior.parameters())\n",
    "        else:\n",
    "            optimizer_params = list(simulator.policy.parameters()) + list(posterior.parameters())\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(optimizer_params)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                                        max_lr=1e-3,\n",
    "                                                        total_steps=num_steps,\n",
    "                                                        pct_start=0.4,\n",
    "                                                        anneal_strategy=\"cos\",\n",
    "                                                        div_factor=25,\n",
    "                                                        final_div_factor=25)\n",
    "        # Collect policy training metrics\n",
    "        metrics = train_policy(simulator=simulator,\n",
    "                               posterior=posterior,\n",
    "                               optimizer=optimizer,\n",
    "                               scheduler=scheduler,\n",
    "                               num_steps=num_steps,\n",
    "                               batch_size=batch_size,\n",
    "                               clip_norm=clip_norm,\n",
    "                               entropy_bonus=entropy_bonus,\n",
    "                               alpha=alpha,\n",
    "                               alpha_decay=alpha_decay,\n",
    "                               verbose=verbose)\n",
    "        \n",
    "    if plot_training: \n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "        ax1.plot(metrics[\"ba_loss\"], linewidth=2)\n",
    "        ax2.plot(metrics[\"grad_norm\"], linewidth=2)\n",
    "        ax1.set_xlabel(\"Step\")\n",
    "        ax1.set_ylabel(\"BA Loss\")\n",
    "        ax1.set_title(\"Training Loss\")\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax2.set_xlabel(\"Step\")\n",
    "        ax2.set_ylabel(\"Gradient Norm\")\n",
    "        ax2.set_title(\"Training Gradient Norm\")\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return simulator, posterior, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe33837",
   "metadata": {},
   "source": [
    "### 6. Example Training Run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de5df12",
   "metadata": {},
   "source": [
    "- Deterministic policy + CouplingFlow posterior with warm-up.  \n",
    "- To skip warm-up, set `num_warmup_steps = 0`. To train only the policy, provide a frozen posterior to `train_joint()`.  \n",
    "- For stochastic policies, setting `entropy_bonus = True` enables `alpha` and `alpha_decay`. To train with a fixed `alpha`, set `alpha_decay = 1`.  \n",
    "- In this example, we set `prior_bounds = policy_bounds = posterior_bounds = 3`, which means that $\\theta$ is sampled uniformly from $[-3, 3]^2$, and both the policy and posterior are trained exactly within this area.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6b21d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_params = {\n",
    "    \"T\": 7,\n",
    "    \"D\": 1,\n",
    "    \"K\": 1,\n",
    "    \"p\": 2,\n",
    "    \"a\": [1.0],\n",
    "    \"m\": 0.001,\n",
    "    \"b\": 0.1,\n",
    "    \"noise_std\": 0.5,\n",
    "    \"prior_bounds\": 3.0,\n",
    "    \"policy_bounds\": 3.0,    \n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    \"posterior_bounds\": 3.0,\n",
    "    \"num_steps\": 3000,\n",
    "    \"num_warmup_steps\": 1000,\n",
    "    \"batch_size\": 256,\n",
    "    \"clip_norm\": 2.0,\n",
    "    \"entropy_bonus\": False,\n",
    "    \"alpha\": 0.0,\n",
    "    \"alpha_decay\": 1.0,\n",
    "    \"device\": None,\n",
    "    \"verbose\": True,\n",
    "    \"plot_training\": True,\n",
    "}\n",
    "\n",
    "sim_trained, posterior_trained, _ = train_joint(policy_type=\"det\", \n",
    "                                                      sim_params=sim_params, \n",
    "                                                      train_params=train_params,\n",
    "                                                      posterior=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f310498f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Roll out the trained policy on randomly sampled theta\n",
    "num_test_theta = 3\n",
    "test_theta = sim_trained.prior().sample((num_test_theta,))\n",
    "designs, outcomes, entropies= sim_trained.run_policy(test_theta)\n",
    "sim_trained.plot_designs(test_theta, designs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4cc463",
   "metadata": {},
   "source": [
    "- We can also plot the trained policy trajectories alongside a Kernel Density Estimate (KDE) of the posterior conditioned on the history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d779ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectory_and_posterior(simulator, \n",
    "                                  posterior,\n",
    "                                  theta, \n",
    "                                  policy_suptitle: str | None = None) -> None:\n",
    "    \n",
    "    assert simulator.p == 2, \"only 2-D plotting supported\"\n",
    "\n",
    "    # Use provided theta\n",
    "    designs, outcomes, _ = simulator.run_policy(theta)\n",
    "\n",
    "    theta_cpu = theta.cpu()\n",
    "    designs_cpu = designs.cpu()\n",
    "    outcomes_cpu = outcomes.cpu()\n",
    "\n",
    "    B = theta.size(0)\n",
    "    num_periods = designs.shape[1]   # T\n",
    "    theta_cpu = theta_cpu.view(B, simulator.K, simulator.p)\n",
    "\n",
    "    # Initialize signal field\n",
    "    bounds = simulator.policy_bounds\n",
    "    grid_size = 100\n",
    "    x_vals = torch.linspace(-bounds, bounds, grid_size)\n",
    "    y_vals = torch.linspace(-bounds, bounds, grid_size)\n",
    "    X, Y = torch.meshgrid(x_vals, y_vals, indexing=\"ij\")\n",
    "    grid_pts = torch.cartesian_prod(x_vals, y_vals)\n",
    "    positions = np.vstack([X.numpy().ravel(), Y.numpy().ravel()])\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        2, B,\n",
    "        figsize=(4*B, 7),\n",
    "        sharex=True,\n",
    "        sharey=True,\n",
    "        squeeze=False,\n",
    "        dpi=300,\n",
    "        constrained_layout=True\n",
    "    )\n",
    "\n",
    "    red = np.array(mcolors.to_rgb(\"#b22222\"))\n",
    "    yellow = np.array(mcolors.to_rgb(\"#ffeb4d\"))\n",
    "\n",
    "    # Row 1: Design trajectories\n",
    "    for b in range(B):\n",
    "        ax = axes[0][b]\n",
    "\n",
    "        # Compute signal field for this trajectory\n",
    "        dist_field = torch.cdist(grid_pts.unsqueeze(0), theta_cpu[b].unsqueeze(0))\n",
    "        a_cpu = simulator.a.cpu()\n",
    "        field = (a_cpu / (simulator.m + dist_field.pow(2))).sum(dim=-1) + simulator.b\n",
    "        Z = torch.log(field.view(grid_size, grid_size)).detach().numpy()\n",
    "\n",
    "        blues = plt.colormaps['Blues']\n",
    "        colors = blues(np.linspace(0, 1, 256))\n",
    "        colors[:, :3] = colors[:, :3] ** 1.25\n",
    "        dark_blues = LinearSegmentedColormap.from_list('dark_blues', colors)\n",
    "\n",
    "        im = ax.contourf(\n",
    "            X.numpy(), Y.numpy(), Z,\n",
    "            levels=40, cmap=dark_blues, alpha=1.0,\n",
    "            vmin=np.min(Z),\n",
    "            vmax=np.max(Z)\n",
    "        )\n",
    "\n",
    "        # Plot design trajectory\n",
    "        for t in range(num_periods):\n",
    "            xi_t = designs_cpu[b, t]\n",
    "            frac = (t + 1) / num_periods\n",
    "            color = red * (1 - frac) + yellow * frac\n",
    "            ax.scatter(\n",
    "                xi_t[:, 0].numpy(), xi_t[:, 1].numpy(),\n",
    "                c=[color], edgecolors=\"k\", s=70\n",
    "            )\n",
    "\n",
    "        ax.set_aspect(\"equal\")\n",
    "        ax.set_xlim(-bounds, bounds)\n",
    "        ax.set_ylim(-bounds, bounds)\n",
    "        ax.grid(True, color=\"lightgray\", linewidth=0.3, alpha=0.3)\n",
    "        ax.set_title(f\"Scenario {b+1}\", fontsize=12)\n",
    "\n",
    "        if b == 0:\n",
    "            ax.set_ylabel(\"Design Trajectory\", fontsize=12)\n",
    "\n",
    "    # Row 2: Corresponding posteriors\n",
    "    n_samples = 500\n",
    "    kde_levels = 300\n",
    "\n",
    "    for b in range(B):\n",
    "        ax = axes[1][b]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            device = next(posterior.parameters()).device\n",
    "            designs_single = designs_cpu[b:b+1].to(device)  # [1, T, D, p]\n",
    "            outcomes_single = outcomes_cpu[b:b+1].to(device)  # [1, T, D]\n",
    "            theta_single = theta[b:b+1].to(device)  # [1, K*p]\n",
    "\n",
    "            # Compute EIG for this trajectory\n",
    "            log_prob = posterior(theta_single, designs_single, outcomes_single)\n",
    "            prior_entropy = simulator.prior().entropy()\n",
    "            eig = (log_prob.mean() + prior_entropy).item()\n",
    "\n",
    "            enc = posterior.history_encoder(designs_single, outcomes_single)\n",
    "            samples = posterior.flow.sample(batch_shape=(n_samples,),\n",
    "                                                conditions=enc.expand(n_samples, -1)).cpu().numpy()\n",
    "\n",
    "            flat_samples = samples.reshape(-1, simulator.p).T\n",
    "\n",
    "            # KDE\n",
    "            kde = gaussian_kde(flat_samples)\n",
    "            Z_post = kde(positions).reshape(X.shape)\n",
    "            Z_post = Z_post / Z_post.max()\n",
    "\n",
    "            contour = ax.contourf(\n",
    "                X.numpy(), Y.numpy(), Z_post,\n",
    "                levels=kde_levels, cmap=\"Purples\", alpha=1.0\n",
    "            )\n",
    "\n",
    "            # Plot true parameters\n",
    "            true_theta = theta_cpu[b].numpy()\n",
    "            for k in range(simulator.K):\n",
    "                scatter = ax.scatter(\n",
    "                    true_theta[k, 0], true_theta[k, 1],\n",
    "                    c='red', s=150, marker='x',\n",
    "                    linewidth=2.5, zorder=10\n",
    "                )\n",
    "\n",
    "            # Add EIG legend\n",
    "            ax.text(0.97, 0.97, f\"Final EIG: {eig:.2f}\",\n",
    "                    transform=ax.transAxes,\n",
    "                    verticalalignment='top',\n",
    "                    horizontalalignment='right',\n",
    "                    bbox=dict(boxstyle='square', facecolor='white', alpha=0.8),\n",
    "                    fontsize=10)\n",
    "    \n",
    "        ax.set_aspect(\"equal\")\n",
    "        ax.set_xlim(-bounds, bounds)\n",
    "        ax.set_ylim(-bounds, bounds)\n",
    "        ax.grid(True, color=\"lightgray\", linewidth=0.3, alpha=0.3)\n",
    "\n",
    "        if b == 0:\n",
    "            ax.set_ylabel(\"Final Posterior\", fontsize=13)\n",
    "\n",
    "    # Colorbar 1: Signal field (for row 1)\n",
    "    row1_axes = axes[0, :]\n",
    "    signal_cbar = fig.colorbar(im, ax=row1_axes, location=\"right\",\n",
    "                                label=\"Log Total Signal\", format=\"%.2f\",\n",
    "                                pad=0.02, shrink=0.9)\n",
    "    signal_cbar.ax.locator_params(nbins=5)\n",
    "    vmin, vmax = im.get_clim()\n",
    "    ticks = np.linspace(vmin, vmax, 6)\n",
    "    signal_cbar.set_ticks(ticks)\n",
    "\n",
    "    # Colorbar 2: Time step (for row 1)\n",
    "    time_cmap = LinearSegmentedColormap.from_list('red_yellow', [red, yellow], N=num_periods)\n",
    "    time_sm = cm.ScalarMappable(cmap=time_cmap, norm=plt.Normalize(vmin=1, vmax=num_periods))\n",
    "    time_sm.set_array([])\n",
    "    time_cbar = fig.colorbar(time_sm, ax=row1_axes, location=\"right\",\n",
    "                            label=\"Time Step\", format=\"%d\",\n",
    "                            pad=0.03, shrink=0.9)\n",
    "    time_cbar.set_ticks(np.arange(1, num_periods + 1))\n",
    "\n",
    "    # Colorbar 3: Posterior density (for row 2)\n",
    "    row2_axes = axes[1, :]\n",
    "    posterior_cbar = fig.colorbar(contour, ax=row2_axes, location=\"right\",\n",
    "                                label=\"Posterior Density\", format=\"%.2f\",\n",
    "                                pad=0.03, shrink=0.9)\n",
    "    vmin, vmax = contour.get_clim()\n",
    "    ticks = np.linspace(vmin, vmax, 6)\n",
    "    posterior_cbar.set_ticks(ticks)\n",
    "\n",
    "    if policy_suptitle:\n",
    "        fig.suptitle(f\"{policy_suptitle} Policy in {B} Scenarios\", fontsize=16)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c5e0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_trajectory_and_posterior(sim_trained, posterior_trained, test_theta, policy_suptitle=\"Deterministic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3d33bb",
   "metadata": {},
   "source": [
    "### 7. Experiment #1: Effect of Entropy Regularization Coefficient ($\\alpha$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89e3a9c",
   "metadata": {},
   "source": [
    "- The experiment code is divided into two stages: training and evaluation.  \n",
    "- In the training stage, the deterministic model (policy + posterior) and the random model (posterior only) are trained once, since these do not depend on `alpha`.  \n",
    "- The stochastic policy is then re-trained for each fixed value in `alpha_values` (with `alpha_decay = 1`), and the results are saved.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a638522c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Training Stage ---------- #\n",
    "\n",
    "alpha_values = [0.0, 0.001, 0.01, 0.1, 1.0]\n",
    "\n",
    "sim_params = {\n",
    "    \"T\": 7,\n",
    "    \"D\": 1,\n",
    "    \"K\": 1,\n",
    "    \"p\": 2,\n",
    "    \"a\": [1.0],\n",
    "    \"m\": 0.001,\n",
    "    \"b\": 0.1,\n",
    "    \"noise_std\": 0.5,\n",
    "    \"prior_bounds\": 3.0,\n",
    "    \"policy_bounds\": 3.0,\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    \"posterior_bounds\": 3.0,\n",
    "    \"num_steps\": 3000,          \n",
    "    \"num_warmup_steps\": 1000,\n",
    "    \"batch_size\": 256,\n",
    "    \"clip_norm\": 2.0,\n",
    "    \"entropy_bonus\": False,\n",
    "    \"alpha\": 0.0,\n",
    "    \"alpha_decay\": 1.0,\n",
    "    \"device\": None,\n",
    "    \"verbose\": True,\n",
    "    \"plot_training\": False,\n",
    "}\n",
    "\n",
    "device = train_params[\"device\"] or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Warmup shared posterior once\n",
    "print(\"Warming up shared posterior...\")\n",
    "simulator_warmup = LocationFinding(\n",
    "    policy=RandomPolicyNet(D=sim_params[\"D\"], p=sim_params[\"p\"], policy_bounds=train_params[\"posterior_bounds\"]),\n",
    "    policy_bounds=train_params[\"posterior_bounds\"],\n",
    "    prior_bounds=train_params[\"posterior_bounds\"],\n",
    "    D=sim_params[\"D\"], K=sim_params[\"K\"], a=sim_params[\"a\"], m=sim_params[\"m\"], b=sim_params[\"b\"],\n",
    "    noise_std=sim_params[\"noise_std\"], T=sim_params[\"T\"], p=sim_params[\"p\"]).to(device)\n",
    "\n",
    "shared_posterior = PosteriorNet(D=sim_params[\"D\"], p=sim_params[\"p\"], K=sim_params[\"K\"], inf_net=\"CouplingFlow\").to(device)\n",
    "shared_posterior = warmup_posterior(simulator_warmup,\n",
    "                                    shared_posterior,\n",
    "                                    num_warmup_steps=train_params[\"num_warmup_steps\"],\n",
    "                                    batch_size=train_params[\"batch_size\"],\n",
    "                                    verbose=train_params[\"verbose\"])\n",
    "\n",
    "# Train random policy once\n",
    "print(\"Training Random Policy...\")\n",
    "sim_rand, post_rand, _ = train_joint(sim_params=sim_params,\n",
    "                                     train_params=train_params,\n",
    "                                     policy_type=\"rand\",\n",
    "                                     posterior=shared_posterior)\n",
    "\n",
    "# Train deterministic policy once\n",
    "print(\"Training Deterministic Policy...\")\n",
    "sim_det, post_det, _ = train_joint(sim_params=sim_params,\n",
    "                                   train_params=train_params,  \n",
    "                                   policy_type=\"det\",\n",
    "                                   posterior=shared_posterior)\n",
    "\n",
    "# Train stochastic policy with different alpha values\n",
    "models_by_alpha = {}\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    print(f\"Training Stochastic Policy (alpha = {alpha})...\")\n",
    "    tp_sto = {**train_params, \"entropy_bonus\": True, \"alpha\": alpha, \"alpha_decay\": 1.0}\n",
    "    sim_sto, post_sto, _ = train_joint(sim_params=sim_params,\n",
    "                                       train_params=tp_sto,\n",
    "                                       policy_type=\"sto\",\n",
    "                                       posterior=shared_posterior)\n",
    "    models_by_alpha[alpha] = {\"sim_sto\": sim_sto, \"post_sto\": post_sto}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a5902c",
   "metadata": {},
   "source": [
    "- For the evaluation stage, we first define a helper function to approximate the average EIG of the trained policy–posterior pair on a testing simulator with specified parameters.  \n",
    "- The parameter `n_mc` controls the number of sampled $\\theta$, while `n_trajectories` specifies the number of trajectories rolled out for each $\\theta$.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ba53bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_policy(policy, \n",
    "                posterior, \n",
    "                sim_params_test,\n",
    "                n_mc=300, \n",
    "                n_trajectories=10,\n",
    "                eval_thetas = None,      # Optionally provide pre-sampled eval thetas \n",
    "                metric: str = \"eig\"      # or \"log_prob\" to evaluate log p(theta | h_T)\" \n",
    "                ):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    # Unwrap test simulation params\n",
    "    (T, D, K, p, a, m, b, noise_std, prior_bounds, policy_bounds) = [sim_params_test[k] for k in [\n",
    "        \"T\", \"D\", \"K\", \"p\", \"a\", \"m\", \"b\",\n",
    "        \"noise_std\", \"prior_bounds\", \"policy_bounds\"]]\n",
    "    \n",
    "    simulator_test = LocationFinding(\n",
    "        policy=policy,\n",
    "        prior_bounds=prior_bounds, \n",
    "        policy_bounds=policy_bounds, \n",
    "        D=D, K=K, a=a, m=m, b=b, noise_std=noise_std, T=T, p=p).to(device)\n",
    "    \n",
    "    metrics = []\n",
    "\n",
    "    # Sample thetas uniformly from policy_bounds\n",
    "    if eval_thetas is None:\n",
    "        eval_thetas = torch.rand(n_mc, K*p, device=device) * (2 * policy_bounds) - policy_bounds     # [n_mc, K*p]\n",
    "\n",
    "    for theta in eval_thetas:\n",
    "        theta_batch = theta.unsqueeze(0)      # [1, K*p]\n",
    "\n",
    "        # Use batched trajectory processing\n",
    "        theta_repeated = theta_batch.repeat(n_trajectories, 1)              # [n_trajectories, K*p]\n",
    "        designs, outcomes, _ = simulator_test.run_policy(theta_repeated)\n",
    "        log_probs = posterior(theta_repeated, designs, outcomes)            # [n_trajectories]\n",
    "\n",
    "        # Average log probability across trajectories for this theta\n",
    "        avg_log_prob = log_probs.mean().item()\n",
    "\n",
    "        if metric == \"log_prob\":\n",
    "            metrics.append(avg_log_prob)\n",
    "\n",
    "        elif metric == \"eig\":\n",
    "            prior_entropy = simulator_test.prior().entropy()\n",
    "            eig_estimate = avg_log_prob + prior_entropy.item()\n",
    "            metrics.append(eig_estimate)\n",
    "    \n",
    "    metrics_np = np.array(metrics)\n",
    "    mean = np.mean(metrics_np)\n",
    "    se = np.std(metrics_np) / np.sqrt(len(metrics_np))\n",
    "\n",
    "    return metrics_np, mean, se"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af593e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Evaluation Stage ---------- #\n",
    "\n",
    "n_mc = 100            \n",
    "n_trajectories = 5 \n",
    "\n",
    "sim_params_test = {\n",
    "    \"T\": 7,\n",
    "    \"D\": 1,\n",
    "    \"K\": 1,\n",
    "    \"p\": 2,\n",
    "    \"a\": [1.0],\n",
    "    \"m\": 0.001,\n",
    "    \"b\": 0.1,\n",
    "    \"noise_std\": 0.5,\n",
    "    \"prior_bounds\": 3.0,\n",
    "    \"policy_bounds\": 3.0,\n",
    "}\n",
    "\n",
    "# Evaluate deterministic policy\n",
    "print(f\"Evaluating Deterministic Policy...\")\n",
    "det_eigs, det_mean, det_se = eval_policy(sim_det.policy, \n",
    "                       post_det, \n",
    "                       sim_params_test,\n",
    "                       n_mc=n_mc, \n",
    "                       n_trajectories=n_trajectories,\n",
    "                       metric=\"eig\")\n",
    "\n",
    "# Evaluate random policy\n",
    "print(f\"Evaluating Random Policy...\")\n",
    "rand_eigs, rand_mean, rand_se = eval_policy(sim_rand.policy, \n",
    "                        post_rand, \n",
    "                        sim_params_test,\n",
    "                        n_mc=n_mc, \n",
    "                        n_trajectories=n_trajectories,\n",
    "                        metric=\"eig\")\n",
    "\n",
    "# Evaluate stochastic policies with different alpha values\n",
    "sto_mean_list = []\n",
    "sto_se_list = []\n",
    "sto_eigs_list = []\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    print(f\"Evaluating Stochastic Policy with alpha={alpha}...\")\n",
    "    sim_sto = models_by_alpha[alpha]['sim_sto']\n",
    "    post_sto = models_by_alpha[alpha]['post_sto']\n",
    "\n",
    "    sto_eigs, sto_mean, sto_se = eval_policy(sim_sto.policy, \n",
    "                           post_sto, \n",
    "                           sim_params_test,\n",
    "                           n_mc=n_mc, \n",
    "                           n_trajectories=n_trajectories, \n",
    "                           metric=\"eig\")\n",
    "    \n",
    "    # Store results\n",
    "    sto_mean_list.append(sto_mean)\n",
    "    sto_se_list.append(sto_se)\n",
    "    sto_eigs_list.append(sto_eigs)\n",
    "    models_by_alpha[alpha]['eigs'] = sto_eigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55db21c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Plot Results ---------- #\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7, 5), dpi=300)\n",
    "\n",
    "# Log scale and ticks\n",
    "ax.set_xscale('log')\n",
    "alpha_plot = [a if a > 0 else 1e-4 for a in alpha_values]\n",
    "xticks = [1e-4, 1e-3, 1e-2, 1e-1, 1e0]\n",
    "ax.set_xticks(xticks)\n",
    "ax.set_xticklabels([\"0\", r\"$10^{-3}$\", r\"$10^{-2}$\", r\"$10^{-1}$\", r\"$10^{0}$\"])\n",
    "ax.grid(True, which='major', alpha=0.3)\n",
    "ax.minorticks_off()\n",
    "\n",
    "# Ensure red/blue line extend to subplot walls\n",
    "left_buffer  = xticks[0] * 0.8\n",
    "right_buffer = xticks[-1] * 1.2\n",
    "ax.set_xlim(left_buffer, right_buffer)\n",
    "xL, xR = ax.get_xlim()\n",
    "\n",
    "# Deterministic policy: mean line + SE band \n",
    "ax.axhline(y=det_mean, color='red', linestyle='-', linewidth=2, label='Deterministic', alpha=0.8)\n",
    "ax.fill_between([xL, xR], det_mean - det_se, det_mean + det_se, color='red', alpha=0.2)\n",
    "\n",
    "# Random policy: mean line + SE band \n",
    "ax.axhline(y=rand_mean, color='blue', linestyle='-', linewidth=2, label='Random', alpha=0.8)\n",
    "ax.fill_between([xL, xR], rand_mean - rand_se, rand_mean + rand_se, color='blue', alpha=0.2)\n",
    "\n",
    "# Stochastic policy: mean line + SE band \n",
    "ax.plot(alpha_plot, sto_mean_list, '-', color='purple', linewidth=2, label='Stochastic')\n",
    "sto_means_arr = np.asarray(sto_mean_list)\n",
    "sto_ses_arr = np.asarray(sto_se_list)  \n",
    "ax.fill_between(alpha_plot, sto_means_arr - sto_ses_arr, sto_means_arr + sto_ses_arr, color='purple', alpha=0.2, zorder=0)\n",
    "\n",
    "# Labels and formatting\n",
    "ax.set_xlabel(r'Entropy Regularization Coefficient ($\\alpha$)', fontsize=12)\n",
    "ax.set_ylabel('Final EIG (BA bound)', fontsize=12)\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "ax.set_title(r'Effect of Entropy Regularization Strength ($T=7$)', fontsize=14, y=1.02)\n",
    "ax.grid(True, alpha=0.3, which='both')\n",
    "ax.legend(loc='best', fontsize=11)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n--- Alpha Experiment Summary ---\")\n",
    "print(f\"Deterministic: {det_mean:.3f} ± {det_se:.3f}\")\n",
    "print(f\"Random:        {rand_mean:.3f} ± {rand_se:.3f}\")\n",
    "print(\"\\nStochastic by Alpha:\")\n",
    "for alpha, mean, se in zip(alpha_values, sto_mean_list, sto_se_list):\n",
    "        print(f\"  α = {alpha:5.3f}: {mean:.3f} ± {se:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9215f9b2",
   "metadata": {},
   "source": [
    "### 8. Experiment #2: Prior Shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7b5807",
   "metadata": {},
   "source": [
    "- Repeat a similar procedure as in previous experiment, but this time our `prior_bounds` = 3.0 is smaller than `posterior_bounds` = `policy_bounds` = 12.0\n",
    "- For the stochastic policy, we proceed with a fixed `alpha` = 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865431a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Training Stage ---------- #\n",
    "\n",
    "sim_params = {\n",
    "    \"T\": 7,\n",
    "    \"D\": 1,\n",
    "    \"K\": 1,\n",
    "    \"p\": 2,\n",
    "    \"a\": [1.0],\n",
    "    \"m\": 0.001,\n",
    "    \"b\": 0.1,\n",
    "    \"noise_std\": 0.5,\n",
    "    \"prior_bounds\": 3.0,\n",
    "    \"policy_bounds\": 12.0,\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    \"posterior_bounds\": 12.0,\n",
    "    \"num_steps\": 3000,\n",
    "    \"num_warmup_steps\": 1000,\n",
    "    \"batch_size\": 256,\n",
    "    \"clip_norm\": 2.0,\n",
    "    \"entropy_bonus\": False,\n",
    "    \"alpha\": 0.0,\n",
    "    \"alpha_decay\": 1.0,\n",
    "    \"device\": None,\n",
    "    \"verbose\": False,\n",
    "    \"plot_training\": False,\n",
    "}\n",
    "\n",
    "device = train_params[\"device\"] or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Warmup shared posterior once with random policy exploring the full posterior_bounds space\n",
    "print(\"Warming up shared posterior...\")\n",
    "simulator_warmup = LocationFinding(\n",
    "    policy=RandomPolicyNet(D=sim_params[\"D\"], p=sim_params[\"p\"], policy_bounds=train_params[\"posterior_bounds\"]),\n",
    "    policy_bounds=train_params[\"posterior_bounds\"],\n",
    "    prior_bounds=train_params[\"posterior_bounds\"],\n",
    "    D=sim_params[\"D\"], K=sim_params[\"K\"], a=sim_params[\"a\"], m=sim_params[\"m\"], b=sim_params[\"b\"],\n",
    "    noise_std=sim_params[\"noise_std\"], T=sim_params[\"T\"], p=sim_params[\"p\"]).to(device)\n",
    "\n",
    "shared_posterior = PosteriorNet(D=sim_params[\"D\"], p=sim_params[\"p\"], K=sim_params[\"K\"], inf_net=\"CouplingFlow\").to(device)\n",
    "shared_posterior = warmup_posterior(simulator_warmup,\n",
    "                                    shared_posterior,\n",
    "                                    num_warmup_steps=train_params[\"num_warmup_steps\"],\n",
    "                                    batch_size=train_params[\"batch_size\"],\n",
    "                                    verbose=train_params[\"verbose\"])\n",
    "\n",
    "# Train deterministic policy\n",
    "print(\"Training Deterministic Policy...\")\n",
    "sim_det, post_det, _ = train_joint(sim_params=sim_params,\n",
    "                                             train_params=train_params,\n",
    "                                             policy_type=\"det\",\n",
    "                                             posterior=shared_posterior)\n",
    "\n",
    "# Train random policy\n",
    "print(\"Training Random Policy...\")\n",
    "sim_rand, post_rand, _ = train_joint(sim_params=sim_params,\n",
    "                                                train_params=train_params,\n",
    "                                                policy_type=\"rand\",\n",
    "                                                posterior=shared_posterior)\n",
    "\n",
    "# Train stochastic policy\n",
    "print(\"Training Stochastic Policy...\")\n",
    "tp_sto = {**train_params, \"entropy_bonus\": True, \"alpha\": 0.01, \"alpha_decay\": 1.0}\n",
    "sim_sto, post_sto, _ = train_joint(sim_params=sim_params,\n",
    "                                             train_params=tp_sto,\n",
    "                                             policy_type=\"sto\",\n",
    "                                             posterior=shared_posterior)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cf7fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Evaluation Stage ---------- #\n",
    "\n",
    "n_mc = 1000            \n",
    "n_trajectories = 10 \n",
    "\n",
    "# Sample shared eval thetas in [-policy_bounds, policy_bounds]\n",
    "shared_eval_thetas = torch.rand(n_mc, sim_params[\"K\"]*sim_params[\"p\"], \n",
    "                                device=device) * (2 * sim_params[\"policy_bounds\"]) - sim_params[\"policy_bounds\"]\n",
    "\n",
    "# Evaluate random policy\n",
    "log_prob_rand, _, _ = eval_policy(sim_rand.policy, \n",
    "                            post_rand,\n",
    "                            sim_params,\n",
    "                            n_mc=n_mc,\n",
    "                            n_trajectories=n_trajectories,\n",
    "                            eval_thetas=shared_eval_thetas,\n",
    "                            metric=\"log_prob\")\n",
    "\n",
    "# Evaluate deterministic policy\n",
    "log_prob_det, _, _ = eval_policy(sim_det.policy, \n",
    "                           post_det,\n",
    "                           sim_params,\n",
    "                           n_mc=n_mc,\n",
    "                           n_trajectories=n_trajectories,\n",
    "                           eval_thetas=shared_eval_thetas,\n",
    "                           metric=\"log_prob\")\n",
    "\n",
    "# Evaluate stochastic policy\n",
    "log_prob_sto, _, _ = eval_policy(sim_sto.policy, \n",
    "                           post_sto,\n",
    "                           sim_params,\n",
    "                           n_mc=n_mc,\n",
    "                           n_trajectories=n_trajectories,\n",
    "                           eval_thetas=shared_eval_thetas,\n",
    "                           metric=\"log_prob\")\n",
    "\n",
    "# Store results\n",
    "results = {\"rand\": log_prob_rand,\n",
    "           \"det\":  log_prob_det,\n",
    "           \"sto\":  log_prob_sto}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8fd0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Plot Results ---------- #\n",
    "\n",
    "def plot_misspecification_heatmaps(eval_thetas, results, sim_params) -> None:\n",
    "\n",
    "    prior_bounds = sim_params['prior_bounds']      # 3.0\n",
    "    policy_bounds = sim_params['policy_bounds']    # 12.0\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3,\n",
    "                            figsize=(12, 4),\n",
    "                            sharex=True,\n",
    "                            sharey=True,\n",
    "                            constrained_layout=True,\n",
    "                            dpi=300)\n",
    "\n",
    "    policy_names = ['Random', 'Deterministic', 'Stochastic']\n",
    "    policy_keys = ['rand', 'det', 'sto']\n",
    "\n",
    "    # Helper function to re-scale the log_probs\n",
    "    def _transform_values(values):\n",
    "        return -np.log(np.abs(values) + 1e-10)\n",
    "\n",
    "    all_transformed = []\n",
    "    for p in policy_keys:\n",
    "        transformed = _transform_values(results[p])\n",
    "        all_transformed.append(transformed)\n",
    "\n",
    "    # Find global min and max for consistent colormap\n",
    "    all_transformed_concat = np.concatenate(all_transformed)\n",
    "    vmin, vmax = np.percentile(all_transformed_concat, [5, 95])\n",
    "    levels = np.linspace(vmin, vmax, 60)\n",
    "\n",
    "    # Create grid for interpolation\n",
    "    x_grid = np.linspace(-policy_bounds, policy_bounds, 200)\n",
    "    y_grid = np.linspace(-policy_bounds, policy_bounds, 200)\n",
    "    X_grid, Y_grid = np.meshgrid(x_grid, y_grid)\n",
    "\n",
    "    for idx, (ax, policy_key, policy_name) in enumerate(zip(axes, policy_keys, policy_names)):\n",
    "        \n",
    "        log_probs = results[policy_key]\n",
    "        transformed_values = _transform_values(log_probs)\n",
    "\n",
    "        XY = np.column_stack([X_grid.ravel(), Y_grid.ravel()])\n",
    "\n",
    "        rbf = RBFInterpolator(eval_thetas.cpu(), transformed_values,\n",
    "                              kernel='gaussian',\n",
    "                              smoothing=0,\n",
    "                              epsilon=3,\n",
    "                              neighbors=3)\n",
    "        \n",
    "        Z = rbf(XY).reshape(X_grid.shape)\n",
    "        Zp = np.clip(Z, vmin, vmax)\n",
    "        im = ax.contourf(X_grid, Y_grid, Zp, levels=levels, cmap='viridis')\n",
    "\n",
    "        # Add boundary square for prior bounds\n",
    "        if policy_key == \"sto\": # Only the stochastic subplot gets a legend \n",
    "            rect = plt.Rectangle((-prior_bounds, -prior_bounds),\n",
    "                                 2*prior_bounds, 2*prior_bounds,\n",
    "                                 fill=False, edgecolor='red', linewidth=1.3,\n",
    "                                 linestyle='-', alpha=0.9, label=f\"Prior (±{prior_bounds:.1f})\")\n",
    "            ax.add_patch(rect)\n",
    "            ax.legend(loc=\"upper right\", fontsize=9, frameon=True)\n",
    "        else:\n",
    "            rect = plt.Rectangle((-prior_bounds, -prior_bounds),\n",
    "                                 2*prior_bounds, 2*prior_bounds,\n",
    "                                 fill=False, edgecolor='red', linewidth=1.3,\n",
    "                                 linestyle='-', alpha=0.9)\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "        # Formatting\n",
    "        ax.set_aspect('equal')\n",
    "        ax.set_xlim(-policy_bounds, policy_bounds)\n",
    "        ax.set_ylim(-policy_bounds, policy_bounds)\n",
    "\n",
    "        tick_vals = np.arange(-policy_bounds, policy_bounds+1e-6, 3.0)\n",
    "        ax.set_xticks(tick_vals)\n",
    "        ax.set_yticks(tick_vals)\n",
    "\n",
    "        ax.set_title(policy_name, fontsize=14)\n",
    "        ax.grid(True, alpha=0.2, color='white', linewidth=0.3)\n",
    "\n",
    "        if idx == 0:\n",
    "            ax.set_ylabel(r'$\\theta_2$', fontsize=12)\n",
    "        ax.set_xlabel(r'$\\theta_1$', fontsize=12)\n",
    "\n",
    "    # Add colorbar\n",
    "    cbar = fig.colorbar(im, ax=axes, location='right',\n",
    "                        label = r'$-\\log\\,|\\,\\mathrm{Posterior\\ Log\\ Prob}\\,|$', format='%.2f',\n",
    "                        pad=0.02, shrink=0.9)\n",
    "    cbar.set_label(r'$-\\log\\,|\\,\\mathrm{Posterior\\ Log\\ Prob}\\,|$', fontsize=12)\n",
    "    cbar.ax.locator_params(nbins=6)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_misspecification_heatmaps(shared_eval_thetas, results, sim_params) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a521ee59",
   "metadata": {},
   "source": [
    "### Experiment #3: Simulator Shift"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65db0fb",
   "metadata": {},
   "source": [
    "- Same procedure, but this time there is no prior shift, and the policies are trained on baseline parameters $\\sigma_y^2 = 0.5$, $a = 1.0$, $b = 0.1$.\n",
    "- As in the previous experiment, the stochastic policy is trained with fixed `alpha`= 0.01.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5432098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Training Stage ---------- #\n",
    "\n",
    "sim_params = {\n",
    "    \"T\": 7,\n",
    "    \"D\": 1,\n",
    "    \"K\": 1,\n",
    "    \"p\": 2,\n",
    "    \"a\": [1.0],\n",
    "    \"m\": 0.001,\n",
    "    \"b\": 0.1,\n",
    "    \"noise_std\": 0.5,\n",
    "    \"prior_bounds\": 3.0,\n",
    "    \"policy_bounds\": 3.0,\n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    \"posterior_bounds\": 3.0,\n",
    "    \"num_steps\": 3000,\n",
    "    \"num_warmup_steps\": 1000,\n",
    "    \"batch_size\": 256,\n",
    "    \"clip_norm\": 2.0,\n",
    "    \"entropy_bonus\": False,\n",
    "    \"alpha\": 0.0,\n",
    "    \"alpha_decay\": 1.0,\n",
    "    \"device\": None,\n",
    "    \"verbose\": False,\n",
    "    \"plot_training\": False,\n",
    "}\n",
    "\n",
    "device = train_params[\"device\"] or torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Warmup shared posterior once with random policy exploring the full posterior_bounds space\n",
    "print(\"Warming up shared posterior...\")\n",
    "simulator_warmup = LocationFinding(\n",
    "    policy=RandomPolicyNet(D=sim_params[\"D\"], p=sim_params[\"p\"], policy_bounds=train_params[\"posterior_bounds\"]),\n",
    "    policy_bounds=train_params[\"posterior_bounds\"],\n",
    "    prior_bounds=train_params[\"posterior_bounds\"],\n",
    "    D=sim_params[\"D\"], K=sim_params[\"K\"], a=sim_params[\"a\"], m=sim_params[\"m\"], b=sim_params[\"b\"],\n",
    "    noise_std=sim_params[\"noise_std\"], T=sim_params[\"T\"], p=sim_params[\"p\"]).to(device)\n",
    "\n",
    "shared_posterior = PosteriorNet(D=sim_params[\"D\"], p=sim_params[\"p\"], K=sim_params[\"K\"], inf_net=\"CouplingFlow\").to(device)\n",
    "shared_posterior = warmup_posterior(simulator_warmup,\n",
    "                                    shared_posterior,\n",
    "                                    num_warmup_steps=train_params[\"num_warmup_steps\"],\n",
    "                                    batch_size=train_params[\"batch_size\"],\n",
    "                                    verbose=train_params[\"verbose\"])\n",
    "\n",
    "# Train deterministic policy\n",
    "print(\"Training Deterministic Policy...\")\n",
    "sim_det, post_det, _ = train_joint(sim_params=sim_params,\n",
    "                                             train_params=train_params,\n",
    "                                             policy_type=\"det\",\n",
    "                                             posterior=shared_posterior)\n",
    "\n",
    "# Train random policy\n",
    "print(\"Training Random Policy...\")\n",
    "sim_rand, post_rand, _ = train_joint(sim_params=sim_params,\n",
    "                                                train_params=train_params,\n",
    "                                                policy_type=\"rand\",\n",
    "                                                posterior=shared_posterior)\n",
    "\n",
    "# Train stochastic policy\n",
    "print(\"Training Stochastic Policy...\")\n",
    "tp_sto = {**train_params, \"entropy_bonus\": True, \"alpha\": 0.01, \"alpha_decay\": 1.0}\n",
    "sim_sto, post_sto, _ = train_joint(sim_params=sim_params,\n",
    "                                             train_params=tp_sto,\n",
    "                                             policy_type=\"sto\",\n",
    "                                             posterior=shared_posterior)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3721767f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Evaluation Stage ---------- #\n",
    "\n",
    "param_ranges = {'noise_std': np.linspace(0.0001, 2.0, 10),\n",
    "                'a': np.linspace(0.0, 4.0, 10),\n",
    "                'b': np.linspace(0.0, 0.8, 10)}\n",
    "\n",
    "n_mc = 100          \n",
    "n_trajectories = 5 \n",
    "\n",
    "# Evaluate each policy on every parameter value in param_ranges\n",
    "sensitivity_results = {}\n",
    "\n",
    "for param_name in param_ranges.keys():\n",
    "    print(f\"Evaluating {param_name.upper()}...\")\n",
    "\n",
    "    param_values = param_ranges[param_name]\n",
    "    sensitivity_results[param_name] = {'values': param_values,\n",
    "                                       'det': {'means': [], 'ses': [], 'all_eigs': []},\n",
    "                                       'rand': {'means': [], 'ses': [], 'all_eigs': []},\n",
    "                                       'sto': {'means': [], 'ses': [], 'all_eigs': []}}\n",
    "\n",
    "    for i, param_value in enumerate(param_values):\n",
    "    \n",
    "        if i % 2 == 0: \n",
    "            print(f\"  [{i+1}/{len(param_values)}] {param_name}={param_value:.3f}\")\n",
    "\n",
    "        test_params = sim_params.copy()\n",
    "        test_params[param_name] = param_value\n",
    "\n",
    "        det_eigs, det_mean, det_se = eval_policy(sim_det.policy,\n",
    "                               post_det,\n",
    "                               test_params,\n",
    "                               n_mc=n_mc,\n",
    "                               n_trajectories=n_trajectories,\n",
    "                               metric=\"eig\")\n",
    "        \n",
    "        sto_eigs, sto_mean, sto_se = eval_policy(sim_sto.policy,\n",
    "                               post_sto,\n",
    "                               test_params,\n",
    "                               n_mc=n_mc,\n",
    "                               n_trajectories=n_trajectories,\n",
    "                               metric=\"eig\")\n",
    "\n",
    "        rand_eigs, rand_mean, rand_se = eval_policy(sim_rand.policy,\n",
    "                               post_rand,\n",
    "                               test_params,\n",
    "                               n_mc=n_mc,\n",
    "                               n_trajectories=n_trajectories,\n",
    "                               metric=\"eig\")\n",
    "\n",
    "        # Store results\n",
    "        sensitivity_results[param_name]['det']['means'].append(det_mean)\n",
    "        sensitivity_results[param_name]['det']['ses'].append(det_se)\n",
    "        sensitivity_results[param_name]['det']['all_eigs'].append(det_eigs)\n",
    "        sensitivity_results[param_name]['rand']['means'].append(rand_mean)\n",
    "        sensitivity_results[param_name]['rand']['ses'].append(rand_se)\n",
    "        sensitivity_results[param_name]['rand']['all_eigs'].append(rand_eigs)\n",
    "        sensitivity_results[param_name]['sto']['means'].append(sto_mean)\n",
    "        sensitivity_results[param_name]['sto']['ses'].append(sto_se)\n",
    "        sensitivity_results[param_name]['sto']['all_eigs'].append(sto_eigs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0966ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Plot Results ---------- #\n",
    "\n",
    "fig, axes = plt.subplots(1, 3,\n",
    "                        figsize=(14, 5),\n",
    "                        sharey=False)\n",
    "\n",
    "x_labels = {'noise_std': r'Noise Std. ($\\sigma_y$)',\n",
    "            'a': r'Signal Strength ($a$)',\n",
    "            'b': r'Background Signal ($b$)'\n",
    "}\n",
    "\n",
    "policy_styles = {'det': {'color': 'red', 'label': 'Deterministic'},\n",
    "                 'sto': {'color': 'purple', 'label': 'Stochastic'},\n",
    "                 'rand': {'color': 'blue', 'label': 'Random'}}\n",
    "\n",
    "for idx, param_name in enumerate(param_ranges.keys()):\n",
    "    ax = axes[idx]\n",
    "\n",
    "    param_values = sensitivity_results[param_name]['values']\n",
    "    baseline_value = sim_params[param_name]\n",
    "\n",
    "    for policy_name in ['det', 'sto', 'rand']:\n",
    "        means = np.array(sensitivity_results[param_name][policy_name]['means'])\n",
    "        ses = np.array(sensitivity_results[param_name][policy_name]['ses']) \n",
    "        style = policy_styles[policy_name]\n",
    "\n",
    "        ax.plot(param_values, means,\n",
    "                color=style['color'],\n",
    "                label=style['label'], linestyle='-',\n",
    "                linewidth=1.5, markersize=8, alpha=0.8)\n",
    "\n",
    "        ax.fill_between(param_values,\n",
    "                        means - ses,\n",
    "                        means + ses,\n",
    "                        color=style['color'], alpha=0.2)\n",
    "\n",
    "    ax.axvline(x=baseline_value, color='black', linestyle='--', alpha=0.7, linewidth=1.5, label=\"Baseline\")\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_xlabel(x_labels[param_name], fontsize=12)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(nbins=6)) \n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    if idx == 2:\n",
    "        ax.legend(loc='best')\n",
    "\n",
    "axes[0].set_ylabel('Final EIG (BA bound)', fontsize=12)\n",
    "\n",
    "plt.suptitle(\"Policy + Posterior Sensitivity to Simulator Shift\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

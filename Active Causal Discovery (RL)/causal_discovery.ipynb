{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "004c8bbe",
   "metadata": {},
   "source": [
    "# Active Causal Discovery Example (RL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a9a716",
   "metadata": {},
   "source": [
    "This notebook demonstrates: \n",
    "- How to generate DAGs, linear Gaussian mechanisms, and train hierarchical (hybrid) policies with the Soft-Critic Actor (SAC) algorithm, where Q-values are enumerated over discrete actions (Q-enumeration). \n",
    "\n",
    "- Plotting code and experiments undertaken to investigate the effect of stochasticity on robustness to prior and misspecification shift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dd7ce5",
   "metadata": {},
   "source": [
    "### 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5832d405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main dependencies\n",
    "import math\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch.distributions import Normal, TransformedDistribution, Categorical, Gamma\n",
    "from torch.distributions.transforms import TanhTransform\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import avici\n",
    "from avici.metrics import shd, classification_metrics, threshold_metrics\n",
    "avici_model = avici.load_pretrained(download=\"scm-v0\")\n",
    "\n",
    "# Set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# For plotting:\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "from matplotlib.colors import Normalize\n",
    "from matplotlib.ticker import MaxNLocator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a38a9c2",
   "metadata": {},
   "source": [
    "### 2. Generate DAGs and Mechanisms (linear Gaussian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73883759",
   "metadata": {},
   "source": [
    "- `prior()`: Generates an Erdős–Rényi-style random DAG, with acyclicality constraints and a topological backbone to ensure weak connectivity.\n",
    "\n",
    "- `LinearGaussianSCM`: Given an adjacency matrix A, samples edge weights normalized by number of parents and noise scale from InverseGamma. Observational values are sampled by following the topological order of the DAG (post-intervention), with optional leading batch dimensions (...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e739561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior(d: int, p: float, device=torch.device('cpu')):\n",
    "    '''Generates an Erdős–Rényi-style DAG, with acyclicality constraints \n",
    "    and a topological backbone to ensure weak connectivity.'''\n",
    "\n",
    "    # Random topological order as a permutation of 0, ..., d-1\n",
    "    order = torch.rand(d, device=device).argsort()          \n",
    "\n",
    "    # Upper triangular Bernoulli mask on device\n",
    "    mask = (torch.rand(d, d, device=device) < p)          \n",
    "    mask = torch.triu(mask, diagonal=1)\n",
    "\n",
    "    # Backbone to ensure weak connectivity in the topological order\n",
    "    if d > 1:\n",
    "        idx = torch.arange(d - 1, device=device)\n",
    "        mask[idx, idx + 1] = True\n",
    "\n",
    "    # inv[u] = i such that order[i] = u\n",
    "    inv = torch.empty_like(order)\n",
    "    inv.scatter_(0, order, torch.arange(d, device=device))\n",
    "\n",
    "    # Permute rows then columns to map from topo-order space to node-label space\n",
    "    A = torch.gather(mask, 0, inv.unsqueeze(1).expand_as(mask))\n",
    "    A = torch.gather(A,    1, inv.unsqueeze(0).expand_as(mask))\n",
    "\n",
    "    return A, order  \n",
    "\n",
    "def adj_to_digraph(A: torch.Tensor) -> nx.DiGraph:\n",
    "    '''Converts adjacency matrix to NetworkX DiGraph (useful for checking properties and plotting)'''\n",
    "\n",
    "    A_cpu = A.to('cpu', dtype=torch.bool)\n",
    "    i, j = A_cpu.nonzero(as_tuple=True)\n",
    "    edges = list(zip(i.tolist(), j.tolist()))\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from(edges)\n",
    "    return G\n",
    "\n",
    "def plot_dag(A: Tensor) -> None:\n",
    "    G = adj_to_digraph(A)\n",
    "    plt.figure(figsize=(5,4))\n",
    "    nx.draw(\n",
    "        G, nx.spring_layout(G, seed=42),\n",
    "        with_labels=True,\n",
    "        node_color=\"lightblue\",\n",
    "        node_size=300,\n",
    "        arrowsize=15,\n",
    "        arrowstyle='->',\n",
    "        font_size=10\n",
    "    )\n",
    "    plt.title(\"Erdős–Rényi-style DAG\")\n",
    "    plt.show()\n",
    "\n",
    "# Check topology\n",
    "d = 5  \n",
    "m = 1.5   # Avg. Num. Parents/Node\n",
    "p = (2 * (d * m - (d - 1))) / ((d - 1) * (d - 2))    # Accounts for the backbone\n",
    "\n",
    "A, order = prior(d=d, p=p, device=device)\n",
    "print(\"Adjacency Matrix (A):\\n\", A.cpu().numpy()*1)\n",
    "print(\"\\nOrder:\", order.cpu().numpy())\n",
    "\n",
    "G = adj_to_digraph(A)\n",
    "print(\"\\nis DAG?\", nx.is_directed_acyclic_graph(G))\n",
    "print(\"is weakly connected?\", nx.is_weakly_connected(G))\n",
    "\n",
    "plot_dag(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95868888",
   "metadata": {},
   "source": [
    "We can also check the effective number of incoming edges (parents) per node, to make sure that our expression for `p` in terms of `m` is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e807c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 5\n",
    "m = 1.5\n",
    "p = (2 * (d * m - (d - 1))) / ((d - 1) * (d - 2))  \n",
    "\n",
    "bin_A = []\n",
    "for _ in range(1000):\n",
    "    A, order = prior(d=d, p=p, device=device)\n",
    "    A_sum = A.sum().cpu().item()\n",
    "    bin_A.append(A_sum)\n",
    "\n",
    "print(f\"p: {p:.3f}\")\n",
    "print(f\"m: {m}\")\n",
    "print(f\"mean(Num. of Parents): {np.mean(bin_A)/d:.2f}\")\n",
    "print(f\"std(Num. of Parents): {np.std(bin_A) / (d**2):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29b5fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearGaussianSCM:\n",
    "    '''Given an adjacency matrix A, samples edge weights normalized by number of parents and noise scale from InverseGamma.\n",
    "        Values are sampled by following the topological order of the DAG (post-intervention), with optional leading batch dimensions.'''\n",
    "    \n",
    "    def __init__(self, \n",
    "                 A: Tensor,\n",
    "                 W_base: Tensor | None = None,\n",
    "                 b: Tensor | None = None,\n",
    "                 sigma: Tensor | None = None,\n",
    "                 device=torch.device('cpu')):\n",
    "        self.device = device\n",
    "        self.d = A.shape[-1]    \n",
    "        A = A.to(device)\n",
    "        \n",
    "        if W_base is None:\n",
    "            # Normalize weights by number of parents to keep noise variance stable\n",
    "            in_degree = A.sum(dim=-2, keepdim=True).clamp_min(1.0)     \n",
    "            coeff_std = (1.0 / torch.sqrt(in_degree)).expand_as(A)    \n",
    "            coefficients = torch.normal(mean=0.0, std=coeff_std)\n",
    "            self.W_base = coefficients\n",
    "        else:\n",
    "            self.W_base = W_base.to(device)\n",
    "        \n",
    "        if b is None:\n",
    "            self.b = torch.zeros(self.d, device=device)  \n",
    "        else:\n",
    "            self.b = b.to(device)\n",
    "        \n",
    "        if sigma is None:\n",
    "            gamma_samples = Gamma(10.0, 1.0).sample((self.d,)).to(device)\n",
    "            self.sigma = 1.0 / gamma_samples\n",
    "        else:\n",
    "            self.sigma = sigma.to(device)\n",
    "\n",
    "    def rsample(self,\n",
    "                A: Tensor,\n",
    "                order: Tensor,\n",
    "                interv_node: Tensor | None = None,\n",
    "                interv_val: Tensor | None = None,\n",
    "                batch_shape: tuple = ()):\n",
    "        A = A.to(self.device)\n",
    "        order = order.to(self.device)\n",
    "                \n",
    "        W_eff = self.W_base * A.to(self.W_base.dtype)\n",
    "        b     = self.b[(None,)*len(batch_shape)].expand(*batch_shape, self.d)\n",
    "        sigma = self.sigma[(None,)*len(batch_shape)].expand(*batch_shape, self.d)\n",
    "\n",
    "        if interv_node is not None:\n",
    "            interv_node = interv_node.to(self.device).bool()\n",
    "        if interv_val is not None:\n",
    "            interv_val = interv_val.to(self.device).float()\n",
    "\n",
    "        eps = torch.randn(*batch_shape, self.d, device=self.device) * sigma\n",
    "        x = torch.zeros(*batch_shape, self.d, device=self.device)\n",
    "        \n",
    "        for k in order:\n",
    "            parents_k = W_eff[:, k]                 # [d]\n",
    "            lin_k = (x * parents_k).sum(dim=-1)     # [*B]\n",
    "\n",
    "            if interv_node is not None and interv_val is not None:\n",
    "                if batch_shape == ():\n",
    "                    is_intervened = interv_node[k]\n",
    "                    x[k] = torch.where(is_intervened, interv_val[k], b[k] + lin_k + eps[k])\n",
    "                else:\n",
    "                    is_intervened = interv_node[..., k]     # [*B]\n",
    "                    x[..., k] = torch.where(is_intervened, interv_val[..., k], b[..., k] + lin_k + eps[..., k])\n",
    "            else:\n",
    "                x[..., k] = b[..., k] + lin_k + eps[..., k]\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e97a955",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test \n",
    "d = 5\n",
    "p = 0.583\n",
    "\n",
    "A, order = prior(d=d, p=p, device=device)\n",
    "print(f\"A: \\n{A.cpu().numpy()*1}\")\n",
    "print(f\"\\nOrder: {order.cpu().numpy()}\")\n",
    "\n",
    "scm = LinearGaussianSCM(A, device=device)\n",
    "\n",
    "# Before intervention\n",
    "x_original = scm.rsample(A, order, batch_shape=(1,))\n",
    "\n",
    "print(\"\\nAvg. values before intervention:\")\n",
    "for i in range(d):\n",
    "    print(f\"   Node {i+1}: {x_original.cpu().mean(dim=-2)[i]:.3f}\")\n",
    "\n",
    "# After intervention (shared across batch)\n",
    "interv_node = torch.tensor([1, 0, 0, 0, 0])\n",
    "interv_val = torch.tensor([1.0, 0, 0, 0, 0])       \n",
    "x_intervened = scm.rsample(A, order, interv_node, interv_val, batch_shape=(1,))\n",
    "print(\"\\nAvg. values after intervention (node 1):\")\n",
    "for i in range(d):\n",
    "    print(f\"   Node {i+1}: {x_intervened.cpu().mean(dim=-2)[i]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93cb766",
   "metadata": {},
   "source": [
    "### 3. Neural Networks (Policies + Critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b512503f",
   "metadata": {},
   "source": [
    "- `HistoryEncoder`: uses a time mask because replayed transitions have different history lengths.\n",
    "  Maps inputs to per-step latents, then applies mean pooling across valid time steps.\n",
    "  Returns zeros if the history is empty.\n",
    "\n",
    "- `TanhGaussianPolicy`: two heads sharing a common history encoding.\n",
    "  The discrete head outputs logits over nodes, then Categorical plus softmax to pick an intervention node.\n",
    "  The continuous head is conditioned on the one-hot node choice and has separate subheads for mean and log standard deviation.\n",
    "\n",
    "- `RandomPolicy`: intervenes on a node with uniform probability, and uniformly samples an intervention value from (-1, 1). \n",
    "\n",
    "- `ObservationPolicy`: Simply returns all zeros for the intervention node and value masks. This is more of a convenience class to decrease the amount of logic in the training loop.\n",
    "\n",
    "- `Critic`: has its own history encoder module, and concatenates the encoding with the one-hot node and value choice\n",
    "  as input to fully connected layers to output a scalar Q-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2694c1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HistoryEncoder(nn.Module):\n",
    "    def __init__(self, d: int, hidden_dim: int = 256, output_dim: int = 64, device=torch.device('cpu')):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.output_dim = output_dim\n",
    "        self.device = device\n",
    "        self.enc_fc1 = nn.Linear(3*d, hidden_dim).to(device)\n",
    "        self.enc_fc2 = nn.Linear(hidden_dim, output_dim).to(device)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim).to(device)\n",
    "\n",
    "    def forward(self,\n",
    "                hist_node: Tensor,               # [*B, t, d]\n",
    "                hist_val: Tensor,                # [*B, t, d]\n",
    "                hist_out: Tensor,                # [*B, t, d]\n",
    "                time_mask: Tensor | None = None  # [*B, t]\n",
    "                ):\n",
    "        *batch_shape, t, _ = hist_node.shape\n",
    "\n",
    "        if t == 0:\n",
    "            return torch.zeros(*batch_shape, self.output_dim, device=self.device)\n",
    "\n",
    "        x = torch.cat([hist_node, hist_val, hist_out], dim=-1)       # [*B, t, 3*d]\n",
    "        h = self.enc_fc1(x)                                          # [*B, t, hidden_dim]\n",
    "        h = self.layer_norm(h)\n",
    "        h = F.relu(h)\n",
    "        out = self.enc_fc2(h)                                        # [*B, t, output_dim]\n",
    "\n",
    "        if time_mask is None:\n",
    "            enc = out.mean(dim=-2)                              \n",
    "        else:\n",
    "            tm = time_mask.float().unsqueeze(-1)                     # [*B, t, 1]\n",
    "            enc = (out * tm).sum(dim=-2) / tm.sum(dim=-2).clamp_min(1.0)\n",
    "\n",
    "        return enc\n",
    "\n",
    "class TanhGaussianPolicy(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d: int,\n",
    "        enc_hidden_dim: int = 256,\n",
    "        enc_output_dim: int = 64,\n",
    "        hidden_dim: int = 256,\n",
    "        min_std: float = 0.01,\n",
    "        max_std: float = 3.0,\n",
    "        init_mean: float = 0.0,\n",
    "        init_std: float = 1.0,\n",
    "        device: torch.device = torch.device(\"cpu\"),\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.device = device\n",
    "        self.history_encoder = HistoryEncoder(d, enc_hidden_dim, enc_output_dim, device)\n",
    "\n",
    "        self.min_log_std = math.log(min_std)\n",
    "        self.max_log_std = math.log(max_std)\n",
    "\n",
    "        # Discrete head\n",
    "        self.disc_fc1 = nn.Linear(enc_output_dim, hidden_dim)\n",
    "        self.disc_fc_mid = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.disc_fc2 = nn.Linear(hidden_dim, d)\n",
    "        self.disc_ln1 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # Continuous heads (conditioned on one-hot intervention node)\n",
    "        self.mean_fc1 = nn.Linear(enc_output_dim + d, hidden_dim)\n",
    "        self.mean_fc_mid = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mean_fc2 = nn.Linear(hidden_dim, 1)\n",
    "        self.mean_ln1 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        self.log_std_fc1 = nn.Linear(enc_output_dim + d, hidden_dim)\n",
    "        self.log_std_fc_mid = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.log_std_fc2 = nn.Linear(hidden_dim, 1)\n",
    "        self.log_std_ln1 = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "        # Initialization\n",
    "        for layer in [self.disc_fc1, self.disc_fc_mid, self.mean_fc1, self.mean_fc_mid, self.log_std_fc1, self.log_std_fc_mid]:\n",
    "            nn.init.xavier_uniform_(layer.weight, gain=nn.init.calculate_gain(\"relu\"))\n",
    "            nn.init.zeros_(layer.bias)\n",
    "\n",
    "        for layer in [self.disc_fc2, self.mean_fc2, self.log_std_fc2]:\n",
    "            nn.init.xavier_uniform_(layer.weight, gain=1.0)\n",
    "            nn.init.zeros_(layer.bias)\n",
    "\n",
    "        self.mean_fc2.bias.data.add_(init_mean)\n",
    "        self.log_std_fc2.bias.data.add_(math.log(max(init_std, 1e-8)))\n",
    "\n",
    "        # Transformations\n",
    "        self._tanh = TanhTransform(cache_size=1)\n",
    "        self.to(device)\n",
    "\n",
    "    def _encode(self, hist_node: Tensor, hist_val: Tensor, hist_out: Tensor, time_mask: Tensor | None):\n",
    "        return self.history_encoder(hist_node, hist_val, hist_out, time_mask)    # [*B, enc_output_dim]\n",
    "\n",
    "    def _disc_logits(self, enc: Tensor):\n",
    "        h = self.disc_ln1(self.disc_fc1(enc))\n",
    "        h = F.relu(h)\n",
    "        h = self.disc_fc_mid(h)\n",
    "        h = F.relu(h)\n",
    "        return self.disc_fc2(h)    # [*B, d]\n",
    "\n",
    "    def _value_params(self, enc: Tensor, one_hot: Tensor):\n",
    "        z = torch.cat([enc, one_hot], dim=-1)           # [*B, enc_output_dim+d]\n",
    "\n",
    "        hm = self.mean_ln1(self.mean_fc1(z))\n",
    "        hm = F.relu(hm)\n",
    "        hm = self.mean_fc_mid(hm)\n",
    "        hm = F.relu(hm)\n",
    "        mean = self.mean_fc2(hm).squeeze(-1)            # [*B]\n",
    "\n",
    "        hs = self.log_std_ln1(self.log_std_fc1(z))\n",
    "        hs = F.relu(hs)\n",
    "        hs = self.log_std_fc_mid(hs)\n",
    "        hs = F.relu(hs) \n",
    "        log_std = self.log_std_fc2(hs).squeeze(-1)      # [*B]\n",
    "        log_std = torch.clamp(log_std, self.min_log_std, self.max_log_std)\n",
    "        std = torch.exp(log_std)                        # [*B]\n",
    "        return mean, std\n",
    "\n",
    "    def _sample_cont(self, enc: Tensor, node_idx: Tensor):\n",
    "        interv_node = F.one_hot(node_idx.long(), num_classes=self.d).float()    # [*B, d]\n",
    "        mean, std = self._value_params(enc, interv_node)\n",
    "        base = Normal(mean, std)\n",
    "        cont_dist = TransformedDistribution(base, [self._tanh])\n",
    "        val = cont_dist.rsample()\n",
    "        interv_val = interv_node * val.unsqueeze(-1)     # [*B, d]\n",
    "        logp_val = cont_dist.log_prob(val)\n",
    "        return interv_node, interv_val, logp_val\n",
    "\n",
    "    def sample_action(self, hist_node: Tensor, hist_val: Tensor, hist_out: Tensor, time_mask: Tensor | None = None):\n",
    "        enc = self._encode(hist_node, hist_val, hist_out, time_mask)\n",
    "        logits = self._disc_logits(enc)\n",
    "        disc_dist = Categorical(logits=logits)\n",
    "        idx = disc_dist.sample()                         # [*B]\n",
    "        logp_idx = disc_dist.log_prob(idx)               # [*B]\n",
    "        interv_node, interv_val, logp_val = self._sample_cont(enc, idx)\n",
    "        info = {\"logits\": logits, \"enc\": enc, \"idx\": idx}\n",
    "        return interv_node, interv_val, logp_idx, logp_val, info\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def mean_action(self, hist_node: Tensor, hist_val: Tensor, hist_out: Tensor, time_mask: Tensor | None = None):\n",
    "        enc = self._encode(hist_node, hist_val, hist_out, time_mask)      # [*B, enc_output_dim]\n",
    "        logits = self._disc_logits(enc)                                   # [*B, d]\n",
    "        idx = torch.argmax(logits, dim=-1)                                # [*B]\n",
    "        interv_node = F.one_hot(idx.long(), num_classes=self.d).float()   # [*B, d]\n",
    "\n",
    "        mean, _ = self._value_params(enc, interv_node)                    # [*B]\n",
    "        val = self._tanh(mean)                                            \n",
    "        interv_val = interv_node * val.unsqueeze(-1)                      # [*B, d]\n",
    "\n",
    "        info = {\"logits\": logits, \"enc\": enc, \"idx\": idx}\n",
    "        return interv_node, interv_val, None, None, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8f35f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomPolicy(nn.Module):\n",
    "    def __init__(self, d: int, eps: float = 1e-3,  device: torch.device = torch.device('cpu')):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.eps = float(eps)\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self):\n",
    "        idx = torch.randint(self.d, (1,), device=self.device).squeeze(0)\n",
    "        val = torch.empty((), device=self.device).uniform_(-1.0 + self.eps, 1.0 - self.eps)\n",
    "        interv_node = torch.zeros(self.d, device=self.device)\n",
    "        interv_node[idx] = 1.0\n",
    "        interv_val = interv_node * val\n",
    "\n",
    "        return interv_node, interv_val\n",
    "\n",
    "class ObservationPolicy(nn.Module):\n",
    "    def __init__(self, d: int, device: torch.device = torch.device('cpu')):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self):\n",
    "        interv_node = torch.zeros(self.d, device=self.device)\n",
    "        interv_val  = torch.zeros(self.d, device=self.device)\n",
    "        return interv_node, interv_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423bbe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, \n",
    "                 d: int, \n",
    "                 enc_hidden_dim: int = 256, \n",
    "                 enc_output_dim: int = 64, \n",
    "                 hidden_dim: int = 256,\n",
    "                 device=torch.device('cpu')):\n",
    "        super().__init__()\n",
    "        self.d = d\n",
    "        self.device = device\n",
    "        self.history_encoder = HistoryEncoder(d, enc_hidden_dim, enc_output_dim, device)\n",
    "\n",
    "        self.q_fc1 = nn.Linear(enc_output_dim + 2*d, hidden_dim).to(device)\n",
    "        self.q_fc2 = nn.Linear(hidden_dim, hidden_dim).to(device)\n",
    "        self.q_fc3 = nn.Linear(hidden_dim, 1).to(device)\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_dim).to(device)\n",
    "\n",
    "        # Initialization\n",
    "        nn.init.xavier_uniform_(self.q_fc1.weight, gain=nn.init.calculate_gain(\"relu\"))\n",
    "        nn.init.zeros_(self.q_fc1.bias)\n",
    "        nn.init.xavier_uniform_(self.q_fc2.weight, gain=nn.init.calculate_gain(\"relu\"))\n",
    "        nn.init.zeros_(self.q_fc2.bias)\n",
    "        nn.init.xavier_uniform_(self.q_fc3.weight, gain=1.0)\n",
    "        nn.init.zeros_(self.q_fc3.bias)\n",
    "\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self,\n",
    "                hist_node: Tensor,        # [*B, t, d]\n",
    "                hist_val: Tensor,         # [*B, t, d]\n",
    "                hist_out: Tensor,         # [*B, t, d]\n",
    "                interv_node: Tensor,      # [*B, d]\n",
    "                interv_val: Tensor,       # [*B, d] \n",
    "                time_mask: Tensor | None = None):\n",
    "        \n",
    "        enc = self.history_encoder(hist_node, hist_val, hist_out, time_mask)\n",
    "        x = torch.cat([enc, interv_node, interv_val], dim=-1)      # [*B, enc_output_dim + 2*d]\n",
    "\n",
    "        h1 = self.q_fc1(x)\n",
    "        h1 = self.layer_norm1(h1)\n",
    "        h1 = F.relu(h1)\n",
    "\n",
    "        h2 = self.q_fc2(h1)\n",
    "        h2 = F.relu(h2)\n",
    "\n",
    "        out = self.q_fc3(h2)         # [*B, 1]\n",
    "        return out.squeeze(-1)       # [*B]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d0e9cf",
   "metadata": {},
   "source": [
    "### 4. Soft Actor-Critic (ReplayBuffer + SAC + Reward Function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930e9277",
   "metadata": {},
   "source": [
    "- `ReplayBuffer`: stores transitions $(s_t, a_t, r_t, s_{t+1}, done_t) = (h_t, (i_t, v_t), r_t, h_{t+1}, done_t)$, and constructs the time mask used in the history encoder.\n",
    "\n",
    "- `SAC`: Soft Actor-Critic with discrete node choice and continuous value (hybrid actions).\n",
    "    - Actor (policy) and twin critics with target networks.\n",
    "    - Entropy regularization with separate coefficients for discrete and continuous actions (optionally tuned online).\n",
    "    - Critic update: minimizes error against target critics, averaging over discrete actions and sampling continuous values from the policy.\n",
    "    - Policy update: maximizes expected Q-value minus entropy terms; optionally updates entropy coefficients; applies\n",
    "    gradient clipping.\n",
    "\n",
    "- `AVICIDeltaReward`: A convenience class for keeping track of the previous and current expected number of correct adjacency matrix (whose difference defines the reward). Requires an `avici_model` to be loaded in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e941642b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"\n",
    "    Stores transitions:\n",
    "      - hist_node, hist_val, hist_out: [T, d]                    (0-padded; first t rows valid)\n",
    "      - time_mask: [T]                                           (0-padded; first t rows valid)\n",
    "      - interv_node, interv_val: [d]\n",
    "      - reward: scalar\n",
    "      - next_hist_node, next_hist_val, next_hist_out: [T, d]     (0-padded; first t+1 rows valid)\n",
    "      - next_time_mask: [T]\n",
    "      - done: scalar bool (=True at last step of trajectory)\n",
    "    \"\"\"\n",
    "    def __init__(self, d: int, T: int, num_obs: int, cap: int, device=torch.device('cpu')):\n",
    "        self.d = d\n",
    "        self.T = T\n",
    "        self.max_len = num_obs + T\n",
    "        self.cap = cap\n",
    "        self.device = device\n",
    "\n",
    "        # Pre-allocate storage on GPU\n",
    "        self.hist_node      = torch.zeros(cap, num_obs + T, d, device=device)\n",
    "        self.hist_val       = torch.zeros(cap, num_obs + T, d, device=device)\n",
    "        self.hist_out       = torch.zeros(cap, num_obs + T, d, device=device)\n",
    "        self.time_mask      = torch.zeros(cap, num_obs + T, device=device)\n",
    "        self.interv_node    = torch.zeros(cap, d, device=device, dtype=torch.bool)\n",
    "        self.interv_val     = torch.zeros(cap, d, device=device)\n",
    "        self.reward         = torch.zeros(cap, device=device)\n",
    "        self.done           = torch.zeros(cap, device=device)\n",
    "        self.next_hist_node = torch.zeros(cap, num_obs + T, d, device=device)\n",
    "        self.next_hist_val  = torch.zeros(cap, num_obs + T, d, device=device)\n",
    "        self.next_hist_out  = torch.zeros(cap, num_obs + T, d, device=device)\n",
    "        self.next_time_mask = torch.zeros(cap, num_obs + T, device=device)\n",
    "\n",
    "        self.idx = 0\n",
    "        self.size = 0\n",
    "    def _advance(self):\n",
    "        self.idx = (self.idx + 1) % self.cap\n",
    "        self.size = min(self.size + 1, self.cap)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def add_transition(self,\n",
    "                       hist_node: Tensor,          # [t, d]\n",
    "                       hist_val: Tensor,           # [t, d]\n",
    "                       hist_out: Tensor,           # [t, d]\n",
    "                       interv_node: Tensor,        # [d]\n",
    "                       interv_val: Tensor,         # [d]\n",
    "                       reward: float | Tensor,   \n",
    "                       next_hist_node: Tensor,     # [t+1, d]\n",
    "                       next_hist_val: Tensor,      # [t+1, d]\n",
    "                       next_hist_out: Tensor,      # [t+1, d]\n",
    "                       done: bool | Tensor):\n",
    "        t = hist_node.shape[-2]\n",
    "        i = self.idx\n",
    "\n",
    "        # Move to device\n",
    "        hist_node = hist_node.to(self.device)\n",
    "        hist_val = hist_val.to(self.device)\n",
    "        hist_out = hist_out.to(self.device)\n",
    "        interv_node = interv_node.to(self.device)\n",
    "        interv_val = interv_val.to(self.device)\n",
    "        next_hist_node = next_hist_node.to(self.device)\n",
    "        next_hist_val = next_hist_val.to(self.device)\n",
    "        next_hist_out = next_hist_out.to(self.device)\n",
    "\n",
    "        # Current history\n",
    "        self.hist_node[i].zero_()\n",
    "        self.hist_val[i].zero_()\n",
    "        self.hist_out[i].zero_()\n",
    "        if t > 0:\n",
    "            self.hist_node[i, :t] = hist_node\n",
    "            self.hist_val[i, :t]  = hist_val\n",
    "            self.hist_out[i, :t]   = hist_out\n",
    "        self.time_mask[i].zero_()\n",
    "        if t > 0:\n",
    "            self.time_mask[i, :t] = 1.0\n",
    "\n",
    "        # Action\n",
    "        self.interv_node[i] = interv_node\n",
    "        self.interv_val[i]  = interv_val\n",
    "\n",
    "        # Reward / done\n",
    "        self.reward[i] = torch.as_tensor(reward, device=self.device)\n",
    "        self.done[i] = torch.as_tensor(done, device=self.device)\n",
    "\n",
    "        # Next history\n",
    "        self.next_hist_node[i].zero_()\n",
    "        self.next_hist_val[i].zero_()\n",
    "        self.next_hist_out[i].zero_()\n",
    "        self.next_hist_node[i, :t+1] = next_hist_node\n",
    "        self.next_hist_val[i, :t+1] = next_hist_val\n",
    "        self.next_hist_out[i, :t+1] = next_hist_out\n",
    "        self.next_time_mask[i].zero_()\n",
    "        self.next_time_mask[i, :t+1] = 1.0\n",
    "\n",
    "        self._advance()\n",
    "\n",
    "    def sample(self, batch_shape: tuple = ()):\n",
    "        \"\"\"Sample uniform random minibatch of transitions.\"\"\"\n",
    "        idx = torch.randint(0, self.size, batch_shape, device=self.device)\n",
    "\n",
    "        batch = {\n",
    "            \"hist_node\":     self.hist_node[idx],\n",
    "            \"hist_val\":      self.hist_val[idx],\n",
    "            \"hist_out\":       self.hist_out[idx],\n",
    "            \"time_mask\":      self.time_mask[idx],\n",
    "            \"interv_node\":   self.interv_node[idx],\n",
    "            \"interv_val\":    self.interv_val[idx],\n",
    "            \"reward\":         self.reward[idx],\n",
    "            \"done\":           self.done[idx],\n",
    "            \"next_hist_node\": self.next_hist_node[idx],\n",
    "            \"next_hist_val\":  self.next_hist_val[idx],\n",
    "            \"next_hist_out\":   self.next_hist_out[idx],\n",
    "            \"next_time_mask\":  self.next_time_mask[idx],\n",
    "        }\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb820dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SAC:\n",
    "    def __init__(self, \n",
    "                 d: int, \n",
    "                 policy: nn.Module, \n",
    "                 critic1: nn.Module, \n",
    "                 critic2: nn.Module,\n",
    "                 lr_actor: float = 1e-4, \n",
    "                 lr_critic: float = 3e-4,\n",
    "                 lr_alpha: float = 3e-4,\n",
    "                 gamma: float = 0.99, \n",
    "                 tau: float = 0.005,\n",
    "                 alpha_d: float = 0.2,\n",
    "                 alpha_c: float = 0.2,\n",
    "                 tune_entropy: bool = False,\n",
    "                 target_entropy_disc: float | None = None,\n",
    "                 target_entropy_cont: float | None = None,\n",
    "                 clip_norm: float | None = None,\n",
    "                 device=torch.device('cpu')):\n",
    "        \n",
    "        self.d = d\n",
    "        self.policy = policy\n",
    "        self.critic1 = critic1\n",
    "        self.critic2 = critic2\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.clip_norm = clip_norm\n",
    "        self.device = device\n",
    "\n",
    "        self.target_critic1 = Critic(self.d, device=device)\n",
    "        self.target_critic2 = Critic(self.d, device=device)\n",
    "        self.target_critic1.load_state_dict(self.critic1.state_dict())\n",
    "        self.target_critic2.load_state_dict(self.critic2.state_dict())\n",
    "\n",
    "        self.policy_optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr_actor)\n",
    "        self.critic1_optimizer = torch.optim.Adam(self.critic1.parameters(), lr=lr_critic)\n",
    "        self.critic2_optimizer = torch.optim.Adam(self.critic2.parameters(), lr=lr_critic)\n",
    "\n",
    "        self.tune_entropy = tune_entropy\n",
    "        if self.tune_entropy:\n",
    "            if target_entropy_disc is None:\n",
    "                target_entropy_disc = 0.25*math.log(self.d)\n",
    "            if target_entropy_cont is None:\n",
    "                target_entropy_cont = -1.0\n",
    "\n",
    "            self.target_entropy_disc = float(target_entropy_disc)\n",
    "            self.target_entropy_cont = float(target_entropy_cont)\n",
    "\n",
    "            self.log_alpha_d = nn.Parameter(torch.log(torch.tensor(alpha_d, device=device).clamp_min(1e-8)), requires_grad=True)\n",
    "            self.log_alpha_c = nn.Parameter(torch.log(torch.tensor(alpha_c, device=device).clamp_min(1e-8)), requires_grad=True)\n",
    "            self.alpha_d_optimizer = torch.optim.Adam([self.log_alpha_d], lr=lr_alpha)\n",
    "            self.alpha_c_optimizer = torch.optim.Adam([self.log_alpha_c], lr=lr_alpha)\n",
    "\n",
    "            self.alpha_d = None\n",
    "            self.alpha_c = None\n",
    "        else:\n",
    "            self.alpha_d = torch.tensor(alpha_d, device=device)\n",
    "            self.alpha_c = torch.tensor(alpha_c, device=device)\n",
    "            self.log_alpha_d = None\n",
    "            self.log_alpha_c = None\n",
    "            self.alpha_d_optimizer = None\n",
    "            self.alpha_c_optimizer = None\n",
    "\n",
    "    def _grad_norm(self, params):\n",
    "        grads = [p.grad.detach().reshape(-1) for p in params if p.grad is not None]\n",
    "        if not grads:\n",
    "            return 0.0\n",
    "        return torch.cat(grads).norm(2).item()\n",
    "\n",
    "    def update_critics(self, batch: dict):\n",
    "        hist_node, hist_val = batch[\"hist_node\"], batch[\"hist_val\"]\n",
    "        hist_out, time_mask = batch[\"hist_out\"], batch[\"time_mask\"]\n",
    "        interv_node, interv_val = batch[\"interv_node\"], batch[\"interv_val\"]\n",
    "        reward, done = batch[\"reward\"], batch[\"done\"]\n",
    "        next_hist_node, next_hist_val = batch[\"next_hist_node\"], batch[\"next_hist_val\"]\n",
    "        next_hist_out, next_time_mask = batch[\"next_hist_out\"], batch[\"next_time_mask\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_enc = self.policy._encode(next_hist_node, \n",
    "                                           next_hist_val, \n",
    "                                           next_hist_out, \n",
    "                                           next_time_mask)\n",
    "            next_logits = self.policy._disc_logits(next_enc)\n",
    "            next_probs_disc = F.softmax(next_logits, dim=-1)\n",
    "            next_log_probs_disc = F.log_softmax(next_logits, dim=-1)\n",
    "\n",
    "            B = hist_node.size(0)   \n",
    "            expected_q_next = torch.zeros(B, device=self.device)\n",
    "            exp_logp_cont_next = torch.zeros(B, device=self.device)\n",
    "\n",
    "            for idx in range(self.d):\n",
    "                idx_batch = torch.full((B,), idx, dtype=torch.long, device=self.device)\n",
    "\n",
    "                # --- Inner expectation over continuous actions ---\n",
    "                next_interv_node, next_interv_val, logp_val_i = self.policy._sample_cont(next_enc, idx_batch)\n",
    "\n",
    "                tq1 = self.target_critic1(next_hist_node, next_hist_val, next_hist_out,\n",
    "                                          next_interv_node, next_interv_val, next_time_mask)\n",
    "                tq2 = self.target_critic2(next_hist_node, next_hist_val, next_hist_out,\n",
    "                                          next_interv_node, next_interv_val, next_time_mask)\n",
    "                tq_min = torch.min(tq1, tq2)\n",
    "                \n",
    "                expected_q_next += next_probs_disc[:, idx] * tq_min\n",
    "                exp_logp_cont_next += next_probs_disc[:, idx] * logp_val_i\n",
    "\n",
    "            exp_logp_disc_next = (next_probs_disc * next_log_probs_disc).sum(dim=-1)\n",
    "\n",
    "            if self.tune_entropy:\n",
    "                alpha_d_t = self.log_alpha_d.exp()\n",
    "                alpha_c_t = self.log_alpha_c.exp()\n",
    "            else:\n",
    "                alpha_d_t = self.alpha_d\n",
    "                alpha_c_t = self.alpha_c\n",
    "\n",
    "            v_next = expected_q_next - alpha_d_t * exp_logp_disc_next - alpha_c_t * exp_logp_cont_next\n",
    "            target_q = reward + (1.0 - done.float()) * self.gamma * v_next\n",
    "\n",
    "        cq1 = self.critic1(hist_node, hist_val, hist_out, interv_node, interv_val, time_mask)\n",
    "        cq2 = self.critic2(hist_node, hist_val, hist_out, interv_node, interv_val, time_mask)\n",
    "\n",
    "        loss1 = F.smooth_l1_loss(cq1, target_q)\n",
    "        loss2 = F.smooth_l1_loss(cq2, target_q)\n",
    "\n",
    "        # Clip gradients\n",
    "        self.critic1_optimizer.zero_grad()\n",
    "        loss1.backward()\n",
    "        if self.clip_norm is not None:\n",
    "            clip_grad_norm_(self.critic1.parameters(), self.clip_norm)\n",
    "        c1_grad_norm = self._grad_norm(self.critic1.parameters())\n",
    "        self.critic1_optimizer.step()\n",
    "\n",
    "        self.critic2_optimizer.zero_grad()\n",
    "        loss2.backward()\n",
    "        if self.clip_norm is not None:\n",
    "            clip_grad_norm_(self.critic2.parameters(), self.clip_norm)\n",
    "        c2_grad_norm = self._grad_norm(self.critic2.parameters())\n",
    "        self.critic2_optimizer.step()\n",
    "\n",
    "        return loss1.item(), loss2.item(), c1_grad_norm, c2_grad_norm\n",
    "\n",
    "    def update_policy(self, batch: dict):\n",
    "        hist_node, hist_val = batch[\"hist_node\"], batch[\"hist_val\"]\n",
    "        hist_out, time_mask = batch[\"hist_out\"], batch[\"time_mask\"]\n",
    "\n",
    "        B = hist_node.shape[0]\n",
    "        \n",
    "        enc = self.policy._encode(hist_node, hist_val, hist_out, time_mask)\n",
    "        logits = self.policy._disc_logits(enc)\n",
    "        disc_probs = F.softmax(logits, dim=-1)\n",
    "        log_probs_disc = F.log_softmax(logits, dim=-1)\n",
    "        avg_logp_disc = torch.zeros(B, device=self.device)\n",
    "        avg_logp_cont = torch.zeros(B, device=self.device)\n",
    "        outer_obj = torch.zeros(B, device=self.device)\n",
    "\n",
    "        if self.tune_entropy:\n",
    "            alpha_d_t = self.log_alpha_d.exp()\n",
    "            alpha_c_t = self.log_alpha_c.exp()\n",
    "        else:\n",
    "            alpha_d_t = self.alpha_d\n",
    "            alpha_c_t = self.alpha_c\n",
    "\n",
    "        for idx in range(self.d):\n",
    "            idx_batch = torch.full((B,), idx, dtype=torch.long, device=self.device)\n",
    "\n",
    "            # --- Inner expectation over continuous actions ---\n",
    "            interv_node_i, interv_val_i, logp_val_i = self.policy._sample_cont(enc, idx_batch)\n",
    "            q1 = self.critic1(hist_node, hist_val, hist_out, interv_node_i, interv_val_i, time_mask)\n",
    "            q2 = self.critic2(hist_node, hist_val, hist_out, interv_node_i, interv_val_i, time_mask)\n",
    "            q_min = torch.min(q1, q2)\n",
    "\n",
    "            inner_obj = q_min - (alpha_c_t.detach() * logp_val_i)\n",
    "\n",
    "            # --- Outer expectation over discrete actions ---\n",
    "            outer_obj += disc_probs[:, idx] * (inner_obj - alpha_d_t.detach() * log_probs_disc[:, idx])\n",
    "\n",
    "            # --- average log probs. for entropy tuning ---\n",
    "            avg_logp_disc += disc_probs[:, idx] * log_probs_disc[:, idx]\n",
    "            avg_logp_cont += disc_probs[:, idx] * logp_val_i\n",
    "\n",
    "        policy_loss = -outer_obj.mean()\n",
    "\n",
    "        self.policy_optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        if self.clip_norm is not None:\n",
    "            clip_grad_norm_(self.policy.parameters(), self.clip_norm)\n",
    "        policy_grad_norm = self._grad_norm(self.policy.parameters())\n",
    "        self.policy_optimizer.step()\n",
    "        \n",
    "        if self.tune_entropy:\n",
    "            alpha_d_loss = -(self.log_alpha_d * (avg_logp_disc.detach() + self.target_entropy_disc)).mean()\n",
    "            alpha_c_loss = -(self.log_alpha_c * (avg_logp_cont.detach() + self.target_entropy_cont)).mean()\n",
    "\n",
    "            self.alpha_d_optimizer.zero_grad()\n",
    "            alpha_d_loss.backward()\n",
    "            self.alpha_d_optimizer.step()\n",
    "\n",
    "            self.alpha_c_optimizer.zero_grad()\n",
    "            alpha_c_loss.backward()\n",
    "            self.alpha_c_optimizer.step()\n",
    "\n",
    "            alpha_d_value = float(self.log_alpha_d.exp().item())\n",
    "            alpha_c_value = float(self.log_alpha_c.exp().item())\n",
    "        else:\n",
    "            alpha_d_value = float(self.alpha_d.item())\n",
    "            alpha_c_value = float(self.alpha_c.item())\n",
    "        \n",
    "        avg_logp_disc = float(avg_logp_disc.mean().item())\n",
    "        avg_logp_cont = float(avg_logp_cont.mean().item())\n",
    "        return policy_loss.item(), avg_logp_disc, avg_logp_cont, alpha_d_value, alpha_c_value, policy_grad_norm\n",
    "\n",
    "    def update_target_networks(self) -> None:\n",
    "        with torch.no_grad():\n",
    "            for tp, p in zip(self.target_critic1.parameters(), self.critic1.parameters()):\n",
    "                tp.copy_(self.tau * p + (1 - self.tau) * tp)\n",
    "            for tp, p in zip(self.target_critic2.parameters(), self.critic2.parameters()):\n",
    "                tp.copy_(self.tau * p + (1 - self.tau) * tp)\n",
    "\n",
    "    def train_step(self, batch: dict):\n",
    "        c1, c2, c1_gn, c2_gn = self.update_critics(batch)\n",
    "        pl, avg_logp_disc, avg_logp_cont, alpha_d_val, alpha_c_val, pol_gn = self.update_policy(batch)\n",
    "        self.update_target_networks()\n",
    "        \n",
    "        return {\n",
    "            \"critic1_loss\": c1,\n",
    "            \"critic2_loss\": c2,\n",
    "            \"policy_loss\": pl,\n",
    "            \"avg_log_prob_disc\": avg_logp_disc,\n",
    "            \"avg_log_prob_cont\": avg_logp_cont,\n",
    "            \"alpha_d\": alpha_d_val,\n",
    "            \"alpha_c\": alpha_c_val,\n",
    "            \"critic1_gn\": c1_gn,\n",
    "            \"critic2_gn\": c2_gn,\n",
    "            \"policy_gn\": pol_gn,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e943ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AviciDeltaReward:\n",
    "    def __init__(self, avici_model):\n",
    "        self.avici_model = avici_model\n",
    "        self.prev_g = None\n",
    "\n",
    "    def reset(self):\n",
    "        self.prev_g = None\n",
    "\n",
    "    def __call__(self,\n",
    "                 A_true: Tensor,      # [d, d]\n",
    "                 hist_nodes: Tensor,  # [t, d] \n",
    "                 hist_out: Tensor     # [t, d] \n",
    "                 ):\n",
    "        x_np = hist_out.detach().cpu().numpy()\n",
    "        interv_np = hist_nodes.detach().cpu().numpy()\n",
    "        A_true_np = A_true.detach().cpu().numpy()\n",
    "\n",
    "        g_prob = self.avici_model(x=x_np, interv=interv_np)\n",
    "        \n",
    "        # Expected number of correct adjacency matrix entries (CAASL-style reward)**\n",
    "        g_t_mat = A_true_np * g_prob + (1.0 - A_true_np) * (1.0 - g_prob)  \n",
    "        g_t = g_t_mat.sum()\n",
    "        \n",
    "        # Telescoping\n",
    "        if self.prev_g is None:\n",
    "            self.prev_g = g_t\n",
    "            return 0.0\n",
    "        r_t = g_t - self.prev_g\n",
    "        self.prev_g = g_t\n",
    "        return r_t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc41886",
   "metadata": {},
   "source": [
    "### 5. SAC Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7093acae",
   "metadata": {},
   "source": [
    "- `train()`:\n",
    "\n",
    "    - For each episode: sample a random DAG and SCM, build an observational prefix of length num_obs, and initialize the AVICI reward with the observations.\n",
    "\n",
    "    - For $t$ in $0,...,T-1$: sample action from `RandomPolicy` during replay buffer warmup; otherwise, sample from ``policy.sample_action``, generate outcomes from the SCM, append to history, compute reward, and store transition $(h_t, (i_t, v_t), r_t, h_{t+1}, done_t)$ in the replay buffer.\n",
    "    \n",
    "    - After each step: if replay buffer warmup is complete, run `updates_per_step` critic updates on minibatches sampled from the replay buffer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3b9e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    d: int = 5, \n",
    "    p: float | None = 0.583,\n",
    "    T: int = 8,\n",
    "    batch_size: int = 128,\n",
    "    num_episodes: int = 5000,\n",
    "    num_obs: int = 20,\n",
    "    print_every: int = 50,\n",
    "    device: torch.device = torch.device(\"cpu\"),\n",
    "    avici_model=None, \n",
    "    tune_entropy: bool = True,\n",
    "    target_entropy_disc: float = 0.5*math.log(5), \n",
    "    target_entropy_cont: float = -1.0,\n",
    "    alpha_d: float = 0.5,\n",
    "    alpha_c: float = 0.5,\n",
    "    updates_per_step: int = 1,\n",
    "    warmup_episodes: int = 500\n",
    "):\n",
    "    assert avici_model is not None, \"avici_model must be provided\"\n",
    "\n",
    "    # Init\n",
    "    policy = TanhGaussianPolicy(d=d, device=device)\n",
    "    rand_policy = RandomPolicy(d=d, device=device)\n",
    "    critic1 = Critic(d=d, device=device)\n",
    "    critic2 = Critic(d=d, device=device)\n",
    "    replay_buffer = ReplayBuffer(d=d, T=T, num_obs=num_obs, cap=T * num_episodes, device=device)\n",
    "\n",
    "    sac = SAC(\n",
    "        d=d,\n",
    "        policy=policy,\n",
    "        critic1=critic1,\n",
    "        critic2=critic2,\n",
    "        lr_actor=1e-4,\n",
    "        lr_critic=3e-4,\n",
    "        lr_alpha=2e-4,      \n",
    "        gamma=1.0,\n",
    "        tau=0.005,\n",
    "        alpha_d=alpha_d,\n",
    "        alpha_c=alpha_c,\n",
    "        tune_entropy=tune_entropy,\n",
    "        target_entropy_disc=target_entropy_disc,\n",
    "        target_entropy_cont=target_entropy_cont,              \n",
    "        clip_norm=5.0,\n",
    "        device=device,\n",
    "    )\n",
    "    training_stats = {\n",
    "        \"critic1_loss\": [], \"critic2_loss\": [], \"policy_loss\": [],\n",
    "        \"eval_critic1_loss\": [], \"eval_critic2_loss\": [], \"eval_policy_loss\": [],\n",
    "        \"critic1_gn\": [], \"critic2_gn\": [], \"policy_gn\": [],\n",
    "        \"episode_reward\": [], \"avg_log_prob_disc\": [], \"avg_log_prob_cont\": [],\n",
    "        \"final_alpha_d\": [], \"final_alpha_c\": [],\n",
    "    }\n",
    "\n",
    "    print(\n",
    "        \"Episode   Train Reward   Policy Loss   Critic1 Loss   Critic2 Loss   \"\n",
    "        \"LogP_disc   LogP_cont    Alpha_d   Alpha_c   Policy GN   Critic1 GN   Critic2 GN\"\n",
    "    )\n",
    "    print(\"-\" * 150)\n",
    "\n",
    "    # Preallocate histories on GPU: [num_obs + T, d]\n",
    "    hist_node = torch.zeros(num_obs + T, d, device=device)\n",
    "    hist_val  = torch.zeros(num_obs + T, d, device=device)\n",
    "    hist_out  = torch.zeros(num_obs + T, d, device=device)\n",
    "\n",
    "    # AVICI reward (CPU)\n",
    "    reward_fn = AviciDeltaReward(avici_model)\n",
    "\n",
    "    for episode in range(1, num_episodes + 1):\n",
    "        A, order = prior(d, p, device=device)\n",
    "        scm = LinearGaussianSCM(A, device=device)\n",
    "\n",
    "        # Observational prefix (no interventions)\n",
    "        if num_obs > 0:\n",
    "            obs_node = torch.zeros(num_obs, d, device=device)\n",
    "            obs_val  = torch.zeros(num_obs, d, device=device)\n",
    "            obs_out  = scm.rsample(A, order, interv_node=None, interv_val=None, batch_shape=(num_obs,))\n",
    "        else:\n",
    "            obs_node = torch.zeros(0, d, device=device)\n",
    "            obs_val  = torch.zeros(0, d, device=device)\n",
    "            obs_out  = torch.zeros(0, d, device=device)\n",
    "        \n",
    "        # Prime AVICI with obs\n",
    "        reward_fn.reset()\n",
    "        _ = reward_fn(A.cpu(), obs_node.cpu(), obs_out.cpu())\n",
    "\n",
    "        # Reset histories and copy prefix\n",
    "        hist_node.zero_(); hist_val.zero_(); hist_out.zero_()\n",
    "        hist_node[:num_obs] = obs_node\n",
    "        hist_val[:num_obs]  = obs_val\n",
    "        hist_out[:num_obs]  = obs_out\n",
    "\n",
    "        episode_losses = {\"critic1\": [], \"critic2\": [], \"policy\": []}\n",
    "        episode_grad_norms = {\"critic1\": [], \"critic2\": [], \"policy\": []}\n",
    "        episode_reward = 0.0\n",
    "        episode_logp_disc = []\n",
    "        episode_logp_cont = []\n",
    "        episode_alpha_d = []\n",
    "        episode_alpha_c = []\n",
    "\n",
    "        for t in range(T):\n",
    "            if episode < warmup_episodes:\n",
    "                interv_node, interv_val = rand_policy() \n",
    "                logp_idx, logp_val, _ = None, None, None \n",
    "            else: \n",
    "                interv_node, interv_val, logp_idx, logp_val, _ = policy.sample_action(hist_node[: num_obs + t],\n",
    "                                                                                  hist_val[: num_obs + t],\n",
    "                                                                                  hist_out[: num_obs + t])\n",
    "            # Environment step\n",
    "            out = scm.rsample(A, order, interv_node=interv_node.bool(), interv_val=interv_val)\n",
    "\n",
    "            # Append to histories\n",
    "            hist_node[num_obs + t] = interv_node\n",
    "            hist_val[num_obs + t]  = interv_val\n",
    "            hist_out[num_obs + t]  = out\n",
    "\n",
    "            # Calculate reward (CPU)\n",
    "            r_t = reward_fn(A.cpu(), hist_node[: num_obs + t + 1].cpu(), hist_out[: num_obs + t + 1].cpu())\n",
    "            episode_reward += r_t\n",
    "\n",
    "            if logp_idx is not None: episode_logp_disc.append(float(logp_idx.item()))\n",
    "            if logp_val is not None: episode_logp_cont.append(float(logp_val.item()))\n",
    "\n",
    "            # Store transition\n",
    "            replay_buffer.add_transition(\n",
    "                hist_node[: num_obs + t],\n",
    "                hist_val[: num_obs + t],\n",
    "                hist_out[: num_obs + t],\n",
    "                interv_node,\n",
    "                interv_val,\n",
    "                r_t,\n",
    "                hist_node[: num_obs + t + 1],\n",
    "                hist_val[: num_obs + t + 1],\n",
    "                hist_out[: num_obs + t + 1],\n",
    "                t == T - 1,\n",
    "            )\n",
    "\n",
    "            # Updates (skip until burn-in complete)\n",
    "            if (episode >= warmup_episodes) and (replay_buffer.size >= batch_size):\n",
    "                for _ in range(updates_per_step):\n",
    "                    batch = replay_buffer.sample(batch_shape=(batch_size,))\n",
    "                    stats = sac.train_step(batch)\n",
    "                    episode_losses[\"critic1\"].append(stats[\"critic1_loss\"])\n",
    "                    episode_losses[\"critic2\"].append(stats[\"critic2_loss\"])\n",
    "                    episode_losses[\"policy\"].append(stats[\"policy_loss\"])\n",
    "                    episode_grad_norms[\"critic1\"].append(stats[\"critic1_gn\"])\n",
    "                    episode_grad_norms[\"critic2\"].append(stats[\"critic2_gn\"])\n",
    "                    episode_grad_norms[\"policy\"].append(stats[\"policy_gn\"])\n",
    "                    episode_alpha_d.append(stats[\"alpha_d\"])\n",
    "                    episode_alpha_c.append(stats[\"alpha_c\"])\n",
    "\n",
    "        # Aggregate stats\n",
    "        training_stats[\"episode_reward\"].append(episode_reward)\n",
    "        training_stats[\"avg_log_prob_disc\"].append(sum(episode_logp_disc) / max(1, len(episode_logp_disc)))\n",
    "        training_stats[\"avg_log_prob_cont\"].append(sum(episode_logp_cont) / max(1, len(episode_logp_cont)))\n",
    "        training_stats[\"critic1_loss\"].append(sum(episode_losses[\"critic1\"]) / max(1, len(episode_losses[\"critic1\"])))\n",
    "        training_stats[\"critic2_loss\"].append(sum(episode_losses[\"critic2\"]) / max(1, len(episode_losses[\"critic2\"])))\n",
    "        training_stats[\"policy_loss\"].append(sum(episode_losses[\"policy\"]) / max(1, len(episode_losses[\"policy\"])))\n",
    "        training_stats[\"critic1_gn\"].append(sum(episode_grad_norms[\"critic1\"]) / max(1, len(episode_grad_norms[\"critic1\"])))\n",
    "        training_stats[\"critic2_gn\"].append(sum(episode_grad_norms[\"critic2\"]) / max(1, len(episode_grad_norms[\"critic2\"])))\n",
    "        training_stats[\"policy_gn\"].append(sum(episode_grad_norms[\"policy\"]) / max(1, len(episode_grad_norms[\"policy\"])))\n",
    "\n",
    "        if sac.tune_entropy and getattr(sac, \"log_alpha_d\", None) is not None:\n",
    "            training_stats[\"final_alpha_d\"].append(float(sac.log_alpha_d.exp().item()))\n",
    "            training_stats[\"final_alpha_c\"].append(float(sac.log_alpha_c.exp().item()))\n",
    "        else:\n",
    "            training_stats[\"final_alpha_d\"].append(float(sac.alpha_d.item()))\n",
    "            training_stats[\"final_alpha_c\"].append(float(sac.alpha_c.item()))\n",
    "\n",
    "        # Print row\n",
    "        if episode % print_every == 0 or episode == 1:\n",
    "            print(\n",
    "                f\"{episode:7d}  {training_stats['episode_reward'][-1]:12.4f}  \"\n",
    "                f\"{training_stats['policy_loss'][-1]:11.4f}  {training_stats['critic1_loss'][-1]:12.4f}  \"\n",
    "                f\"{training_stats['critic2_loss'][-1]:12.4f}      {training_stats['avg_log_prob_disc'][-1]:9.4f}   \"\n",
    "                f\"{training_stats['avg_log_prob_cont'][-1]:9.4f}    \"\n",
    "                f\"{training_stats['final_alpha_d'][-1]:7.4f}   {training_stats['final_alpha_c'][-1]:7.4f}  \"\n",
    "                f\"{training_stats['policy_gn'][-1]:9.4f}  {training_stats['critic1_gn'][-1]:11.4f} {training_stats['critic2_gn'][-1]:11.4f}\"\n",
    "            )\n",
    "\n",
    "    return training_stats, sac, replay_buffer, policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62813b1",
   "metadata": {},
   "source": [
    "### 6. Example Training Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64c7c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stats, sac, replay_buffer, trained_policy = train(\n",
    "    d=5,\n",
    "    T=8,\n",
    "    p=0.583,\n",
    "    batch_size=128,\n",
    "    num_episodes=5000,    \n",
    "    num_obs=20,\n",
    "    print_every=25,\n",
    "    device=torch.device(\"cuda\"),\n",
    "    avici_model=avici_model,\n",
    "    tune_entropy=True,\n",
    "    target_entropy_disc=0.25*math.log(5),  \n",
    "    target_entropy_cont=-1.0,             \n",
    "    alpha_d=0.5,\n",
    "    alpha_c=0.5,  \n",
    "    updates_per_step=1,\n",
    "    warmup_episodes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00d3467",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plots training statistics ---\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 16))\n",
    "fig.suptitle('SAC Training Metrics', fontsize=16)\n",
    "x = np.arange(1, len(training_stats['episode_reward']) + 1)\n",
    "\n",
    "# Row 0\n",
    "axes[0, 0].plot(x, training_stats['episode_reward'], 'b-', alpha=0.7)\n",
    "axes[0, 0].set_title('Training Episode Reward')\n",
    "axes[0, 0].set_xlabel('Episodes')\n",
    "axes[0, 0].set_ylabel('Reward')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Average log-probs: discrete and continuous\n",
    "axes[0, 1].plot(x, training_stats['avg_log_prob_disc'], 'purple', alpha=0.7, label='disc')\n",
    "axes[0, 1].plot(x, training_stats['avg_log_prob_cont'], 'teal',   alpha=0.7, label='cont')\n",
    "axes[0, 1].set_title('Average Log Probability')\n",
    "axes[0, 1].set_xlabel('Episodes')\n",
    "axes[0, 1].set_ylabel('Log Prob')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Entropy coefficients\n",
    "axes[0, 2].plot(x, training_stats['final_alpha_d'], 'm-', alpha=0.7, label='alpha_d')\n",
    "axes[0, 2].plot(x, training_stats['final_alpha_c'], 'c-', alpha=0.7, label='alpha_c')\n",
    "axes[0, 2].set_title('Entropy Coefficients')\n",
    "axes[0, 2].set_xlabel('Episodes')\n",
    "axes[0, 2].set_ylabel('Value')\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "axes[0, 2].legend()\n",
    "\n",
    "# Row 1: losses\n",
    "axes[1, 0].plot(x, training_stats['policy_loss'], 'r-', alpha=0.7)\n",
    "axes[1, 0].set_title('Policy Loss')\n",
    "axes[1, 0].set_xlabel('Episodes')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].plot(x, training_stats['critic1_loss'], 'r-', alpha=0.7)\n",
    "axes[1, 1].set_title('Critic 1 Loss')\n",
    "axes[1, 1].set_xlabel('Episodes')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 2].plot(x, training_stats['critic2_loss'], 'r-', alpha=0.7)\n",
    "axes[1, 2].set_title('Critic 2 Loss')\n",
    "axes[1, 2].set_xlabel('Episodes')\n",
    "axes[1, 2].set_ylabel('Loss')\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Row 3: grad norms\n",
    "axes[2, 0].plot(x, training_stats['policy_gn'], 'k-', alpha=0.7)\n",
    "axes[2, 0].set_title('Policy Grad Norm')\n",
    "axes[2, 0].set_xlabel('Episodes')\n",
    "axes[2, 0].set_ylabel('L2 Norm')\n",
    "axes[2, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2, 1].plot(x, training_stats['critic1_gn'], 'k-', alpha=0.7)\n",
    "axes[2, 1].set_title('Critic 1 Grad Norm')\n",
    "axes[2, 1].set_xlabel('Episodes')\n",
    "axes[2, 1].set_ylabel('L2 Norm')\n",
    "axes[2, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[2, 2].plot(x, training_stats['critic2_gn'], 'k-', alpha=0.7)\n",
    "axes[2, 2].set_title('Critic 2 Grad Norm')\n",
    "axes[2, 2].set_xlabel('Episodes')\n",
    "axes[2, 2].set_ylabel('L2 Norm')\n",
    "axes[2, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f80c9e1",
   "metadata": {},
   "source": [
    "### 7. Visualize Baseline Policy Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6672563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_interventions(d: int,\n",
    "                       T: int,\n",
    "                       A: Tensor,\n",
    "                       interv_nodes: Tensor,\n",
    "                       interv_values: Tensor,\n",
    "                       scm,\n",
    "                       policy_title: str = \"Trained\") -> None:\n",
    "    \"\"\"Plots trajectories given a DAG, SCM, and chosen interventions\"\"\"\n",
    "    G = adj_to_digraph(A)\n",
    "\n",
    "    total_panels = T + 1\n",
    "    max_cols = 4\n",
    "    ncols = min(total_panels, max_cols)\n",
    "    nrows = math.ceil(total_panels / ncols)\n",
    "\n",
    "    fig_width  = 4 * ncols\n",
    "    fig_height = 4 * nrows\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows, ncols, figsize=(fig_width, fig_height),\n",
    "        squeeze=False, constrained_layout=True, dpi=300\n",
    "    )\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    interv_nodes_cpu  = interv_nodes.detach().cpu()\n",
    "    interv_values_cpu = interv_values.detach().cpu()\n",
    "\n",
    "    assert interv_nodes_cpu.shape == (T, d), f\"interv_nodes shape {tuple(interv_nodes_cpu.shape)} != (T, d)\"\n",
    "    assert interv_values_cpu.shape == (T, d), f\"interv_values shape {tuple(interv_values_cpu.shape)} != (T, d)\"\n",
    "\n",
    "    # Map adjacency indices to graph node idx\n",
    "    nodes_G = set(G.nodes())\n",
    "    if nodes_G == set(range(d)):\n",
    "        idx_to_node = list(range(d))\n",
    "        node_labels = {i: str(i + 1) for i in range(d)}\n",
    "    elif nodes_G == set(range(1, d + 1)):\n",
    "        idx_to_node = list(range(1, d + 1))\n",
    "        node_labels = {i: str(i) for i in range(1, d + 1)}\n",
    "    else:\n",
    "        raise ValueError(\"Graph nodes must be indexed as 0..d-1 or 1..d.\")\n",
    "\n",
    "    nodelist = [idx_to_node[i] for i in range(d)]\n",
    "    neutral_gray = \"#d9d9d9\"\n",
    "\n",
    "    # Colormap\n",
    "    base_cmap = mpl.colormaps[\"managua\"].reversed()\n",
    "    def lighten(c, factor=0.3):\n",
    "        r, g, b, a = c\n",
    "        return (r + (1 - r) * factor, g + (1 - g) * factor, b + (1 - b) * factor, a)\n",
    "    colors_list = [lighten(base_cmap(i), factor=0.35) for i in np.linspace(0, 1, 256)]\n",
    "    cmap = colors.LinearSegmentedColormap.from_list(\"light_managua_rev\", colors_list)\n",
    "    norm = Normalize(vmin=-1.0, vmax=1.0)\n",
    "\n",
    "    node_size = 600\n",
    "    pos = nx.kamada_kawai_layout(G) \n",
    "\n",
    "    # Edge indices from A\n",
    "    rows, cols = torch.nonzero(A.detach().cpu(), as_tuple=True)\n",
    "    edge_indices = list(zip(rows.tolist(), cols.tolist()))\n",
    "\n",
    "    # First panel: DAG structure with edge weights and noise scales\n",
    "    ax = axes[0]\n",
    "    ax.set_facecolor(\"white\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    for spine in ax.spines.values():\n",
    "        spine.set_visible(True)\n",
    "        spine.set_linewidth(0.6)\n",
    "        spine.set_edgecolor(\"#cccccc\")\n",
    "    ax.margins(0.08)\n",
    "\n",
    "    # Draw nodes in neutral gray\n",
    "    node_colors = [neutral_gray] * d\n",
    "    nx.draw_networkx_nodes(\n",
    "        G, pos, ax=ax, nodelist=nodelist,\n",
    "        node_color=node_colors, node_size=node_size,\n",
    "        edgecolors=\"black\", linewidths=0.5)\n",
    "    nx.draw_networkx_labels(G, pos, ax=ax, labels=node_labels, font_size=14)\n",
    "\n",
    "    # Draw all edges as solid\n",
    "    nx.draw_networkx_edges(\n",
    "        G, pos, ax=ax, edgelist=list(G.edges()),\n",
    "        arrows=True, arrowstyle=\"-|>\", arrowsize=17,\n",
    "        width=1, min_target_margin=12.5, connectionstyle=\"arc3,rad=0.05\")\n",
    "\n",
    "    # Draw edge weights\n",
    "    edge_labels = {(idx_to_node[i], idx_to_node[j]): f\"{float(scm.W_base[i, j]):.2f}\"\n",
    "                    for (i, j) in edge_indices}\n",
    "    nx.draw_networkx_edge_labels(\n",
    "        G, pos, ax=ax, edge_labels=edge_labels,\n",
    "        font_size=11, font_color=\"black\", label_pos=0.35, alpha=1.0)\n",
    "\n",
    "    # Add noise scales in red next to each node\n",
    "    for i, node_id in enumerate(nodelist):\n",
    "        x, y = pos[node_id]\n",
    "        sigma_val = float(scm.sigma[i])\n",
    "        ax.text(x, y + 0.2, f\"{sigma_val:.2f}\",\n",
    "                fontsize=11, color=\"red\",\n",
    "                ha='center', va='center')\n",
    "\n",
    "    ax.set_title(\"DAG Structure\", fontsize=18)\n",
    "\n",
    "    nodeid_to_idx = {nodelist[i]: i for i in range(d)}\n",
    "    base_colors = [neutral_gray] * d\n",
    "\n",
    "    for t in range(T):\n",
    "        ax = axes[t + 1]\n",
    "        row = interv_nodes_cpu[t]\n",
    "        has_intervention = bool(torch.any(row > 0))\n",
    "\n",
    "        if not has_intervention:\n",
    "            target_node = None\n",
    "            colors_t = base_colors\n",
    "        else:\n",
    "            intervened_idx = int(torch.argmax(row).item())\n",
    "            target_node = idx_to_node[intervened_idx]\n",
    "            colors_t = base_colors[:]\n",
    "            val = float(np.clip(float(interv_values_cpu[t, intervened_idx]), -1.0, 1.0))\n",
    "            k = nodeid_to_idx[target_node]\n",
    "            colors_t[k] = cmap(norm(val))\n",
    "\n",
    "        # Draw nodes and labels\n",
    "        nx.draw_networkx_nodes(\n",
    "            G, pos, ax=ax, nodelist=nodelist,\n",
    "            node_color=colors_t, \n",
    "            node_size=node_size, edgecolors=\"black\", linewidths=0.5\n",
    "        )\n",
    "        nx.draw_networkx_labels(G, pos, ax=ax, labels=node_labels, font_size=14)\n",
    "\n",
    "        # Edges\n",
    "        if target_node is None:\n",
    "            edges_dashed = []\n",
    "            edges_solid  = list(G.edges())\n",
    "        else:\n",
    "            edges_dashed = [(u, v) for (u, v) in G.edges() if v == target_node]\n",
    "            edges_solid  = [(u, v) for (u, v) in G.edges() if v != target_node]\n",
    "\n",
    "        if edges_solid:\n",
    "            nx.draw_networkx_edges(\n",
    "                G, pos, ax=ax, edgelist=edges_solid,\n",
    "                arrows=True, arrowstyle=\"-|>\", arrowsize=17,\n",
    "                width=1, min_target_margin=12.5, connectionstyle=\"arc3,rad=0.05\",\n",
    "            )\n",
    "        if edges_dashed:\n",
    "            nx.draw_networkx_edges(\n",
    "                G, pos, ax=ax, edgelist=edges_dashed,\n",
    "                arrows=True, arrowstyle=\"-|>\", arrowsize=17,\n",
    "                width=1.2, style=(0, (3, 3)), min_target_margin=12.5, connectionstyle=\"arc3,rad=0.05\"\n",
    "            )\n",
    "\n",
    "        ax.set_title(rf\"$t={t+1}$\", fontsize=18)\n",
    "\n",
    "    for ax in axes[total_panels:]:\n",
    "        ax.set_visible(False)\n",
    "\n",
    "    sm = mpl.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
    "    sm.set_array([])\n",
    "    cbar = fig.colorbar(\n",
    "        sm,\n",
    "        ax=axes[:total_panels],\n",
    "        location=\"bottom\",\n",
    "        orientation=\"horizontal\",\n",
    "        anchor=(1.0, 0.0),\n",
    "        shrink=0.25,\n",
    "        pad=0.03,\n",
    "        aspect=20,\n",
    "    )\n",
    "    cbar.ax.tick_params(labelsize=14)\n",
    "    cbar.set_label(\"Intervention value\", fontsize=18, labelpad=7)\n",
    "\n",
    "    title_str = rf\"{policy_title} Policy Trajectory ($T={T}$)\"\n",
    "    fig.suptitle(title_str, fontsize=22, y=1.05)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ae0b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_adj_heatmap(pred_list, true, titles=None, cmap=\"viridis\", dpi=300):\n",
    "    \"\"\"Plots the adjacency matrix heatmap of predicted edge probabilities (AVICI output).\n",
    "    Loops over a list of predictions, plotting each side-by-side until the final subplot, \n",
    "    which plots the ground truth adjacency matrix\"\"\"\n",
    "\n",
    "    # Convert inputs to numpy\n",
    "    preds = [p.detach().cpu().numpy() if isinstance(p, torch.Tensor) else np.asarray(p) for p in pred_list]\n",
    "    T = true.detach().cpu().numpy() if isinstance(true, torch.Tensor) else np.asarray(true)\n",
    "\n",
    "    # Shared color scale across all panels\n",
    "    vmax = float(max([p.max() for p in preds] + [T.max()]))\n",
    "    vmin = 0.0\n",
    "\n",
    "    if titles is None:\n",
    "        titles = [f\"Pred {i+1}\" for i in range(len(preds))]\n",
    "    titles = list(titles) + [\"True\"]\n",
    "\n",
    "    num_panels = len(preds) + 1\n",
    "    cell_size = 0.35\n",
    "    fig_w = num_panels * d * cell_size + 1.2\n",
    "    fig_h = d * cell_size + 0.8\n",
    "\n",
    "    fig, axes = plt.subplots(\n",
    "        1, num_panels,\n",
    "        figsize=(fig_w, fig_h), dpi=dpi,\n",
    "        constrained_layout=True)\n",
    "\n",
    "    tick_pos = np.arange(d)\n",
    "    tick_lbl = [str(i+1) for i in range(d)]\n",
    "\n",
    "    for j, (ax, data, title) in enumerate(zip(axes[:-1], preds, titles[:-1])):\n",
    "        ax.imshow(data, cmap=cmap, vmin=vmin, vmax=vmax,\n",
    "                  interpolation=\"nearest\", aspect=\"equal\")\n",
    "        ax.set_title(title, fontsize=9)\n",
    "\n",
    "        ax.set_xlim(-0.5, d-0.5)\n",
    "        ax.set_ylim(d-0.5, -0.5)  \n",
    "\n",
    "        if j == 0:\n",
    "            ax.set_yticks(tick_pos)\n",
    "            ax.set_yticklabels(tick_lbl, fontsize=8)\n",
    "        else:\n",
    "            ax.set_yticks([])\n",
    "\n",
    "        ax.set_xticks(tick_pos)\n",
    "        ax.set_xticklabels(tick_lbl, fontsize=8)\n",
    "\n",
    "        for s in ax.spines.values():\n",
    "            s.set_linewidth(0.6)\n",
    "            s.set_edgecolor(\"black\")\n",
    "\n",
    "    # Plot truth in the last panel\n",
    "    ax_true = axes[-1]\n",
    "    ax_true.imshow(T, cmap=cmap, vmin=vmin, vmax=vmax,\n",
    "                   interpolation=\"nearest\", aspect=\"equal\")\n",
    "    ax_true.set_title(titles[-1], fontsize=9)\n",
    "    ax_true.set_xlim(-0.5, d-0.5)\n",
    "    ax_true.set_ylim(d-0.5, -0.5)\n",
    "    ax_true.set_xticks(tick_pos)\n",
    "    ax_true.set_xticklabels(tick_lbl, fontsize=8)\n",
    "    ax_true.set_yticks([])\n",
    "\n",
    "    for s in ax_true.spines.values():\n",
    "        s.set_linewidth(0.6)\n",
    "        s.set_edgecolor(\"black\")\n",
    "\n",
    "    # Shared colorbar on the right\n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=plt.Normalize(vmin=vmin, vmax=vmax))\n",
    "    sm.set_array([])\n",
    "    cbar = fig.colorbar(\n",
    "        sm,\n",
    "        ax=axes.ravel().tolist(),\n",
    "        location=\"right\",\n",
    "        orientation=\"vertical\",\n",
    "        shrink=0.7,\n",
    "        pad=0.02,\n",
    "        aspect=15\n",
    "    )\n",
    "    cbar.set_label(\"Edge Probability\", fontsize=9, labelpad=5)\n",
    "    cbar.ax.tick_params(labelsize=8)\n",
    "\n",
    "    fig.text(0.5, 0.04, \"Child Node\", ha=\"center\", va=\"top\", fontsize=9)\n",
    "    fig.text(-0.005, 0.5, \"Parent Node\", ha=\"right\", va=\"center\", fontsize=9, rotation=\"vertical\")\n",
    "\n",
    "    fig.suptitle(\"Final Edge Predictions\", fontsize=10, y=1.0)\n",
    "\n",
    "    return fig, axes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5957f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_policy(policy,\n",
    "                 scm,          \n",
    "                 A: Tensor,          \n",
    "                 order: Tensor,  \n",
    "                 d: int,\n",
    "                 T: int,\n",
    "                 device: torch.device,\n",
    "                 obs_nodes: Tensor | None = None,  \n",
    "                 obs_vals:  Tensor | None = None,  \n",
    "                 obs_out:   Tensor | None = None,\n",
    "                 action_mode: str = \"sample\"):  # \"sample\" or \"mean\"\n",
    "      \"\"\"For a given DAG, SCM, policy, and observational prefix, roll out a trajectory \n",
    "      of interventions and outcomes under the policy\"\"\"\n",
    "\n",
    "      interv_nodes = torch.zeros(T, d, device=device)\n",
    "      interv_vals  = torch.zeros(T, d, device=device)\n",
    "      interv_out   = torch.zeros(T, d, device=device)\n",
    "\n",
    "      was_training = getattr(policy, \"training\", None)\n",
    "      if hasattr(policy, \"eval\"): policy.eval()\n",
    "\n",
    "      for t in range(T):\n",
    "          if obs_out is not None:\n",
    "              hist_nodes = torch.cat([obs_nodes, interv_nodes[:t]], dim=0)\n",
    "              hist_vals  = torch.cat([obs_vals,  interv_vals[:t]],  dim=0)\n",
    "              hist_out   = torch.cat([obs_out,   interv_out[:t]],   dim=0)\n",
    "          else:\n",
    "              hist_nodes = interv_nodes[:t]\n",
    "              hist_vals  = interv_vals[:t]\n",
    "              hist_out   = interv_out[:t]\n",
    "\n",
    "          hist_nodes = hist_nodes.to(device)\n",
    "          hist_vals = hist_vals.to(device)\n",
    "          hist_out = hist_out.to(device)\n",
    "\n",
    "          if action_mode == \"mean\" and hasattr(policy, \"mean_action\"):\n",
    "              interv_node_t, interv_val_t, _, _, _ = policy.mean_action(hist_nodes, hist_vals, hist_out)\n",
    "          else:\n",
    "              # default to sample\n",
    "              if hasattr(policy, \"sample_action\"):\n",
    "                  interv_node_t, interv_val_t, _, _, _ = policy.sample_action(hist_nodes, hist_vals, hist_out)\n",
    "              else:\n",
    "                  interv_node_t, interv_val_t = policy()\n",
    "\n",
    "          out_t = scm.rsample(A, order, interv_node=interv_node_t, interv_val=interv_val_t)\n",
    "\n",
    "          interv_nodes[t] = interv_node_t\n",
    "          interv_vals[t]  = interv_val_t\n",
    "          interv_out[t]   = out_t\n",
    "\n",
    "      # restore training flag\n",
    "      if was_training is not None and hasattr(policy, \"train\"):\n",
    "          policy.train(was_training)\n",
    "\n",
    "      return interv_nodes, interv_vals, interv_out\n",
    "\n",
    "\n",
    "def _avici_metrics(interv_nodes, interv_outs, A_np, avici_model):\n",
    "    \"\"\"Helper function to pass intervention node masks and outcomes\n",
    "    to the AVICI model, using AVICI's native classification metrics to evaluate\n",
    "    the trajectory\"\"\"\n",
    "\n",
    "    to_cpu = lambda x: x.detach().cpu() if torch.is_tensor(x) else x\n",
    "    nodes_np = to_cpu(interv_nodes).numpy()\n",
    "    outs_np  = to_cpu(interv_outs).numpy()\n",
    "\n",
    "    g_prob   = avici_model(x=outs_np, interv=nodes_np)\n",
    "    pred_adj = (g_prob > 0.5).astype(int)\n",
    "\n",
    "    shd_val   = shd(A_np, pred_adj)\n",
    "    f1_val    = classification_metrics(A_np, pred_adj)['f1']\n",
    "    auroc_val = threshold_metrics(A_np, g_prob)['auroc']\n",
    "    return g_prob, shd_val, f1_val, auroc_val\n",
    "\n",
    "def eval_policy(policy,\n",
    "                avici_model,  \n",
    "                d: int,\n",
    "                T: int,\n",
    "                device = torch.device(\"cpu\"),\n",
    "                p: float | None = 0.583,\n",
    "                num_obs: int = 50,\n",
    "                visualize_interv: bool = True):\n",
    "    policy = policy.to(device)\n",
    "    \n",
    "    # SCM + graph\n",
    "    A, order = prior(d, p, device=device)\n",
    "    scm = LinearGaussianSCM(A, device=device)\n",
    "\n",
    "    # --- Observational prefix (no interventions) ---\n",
    "    if num_obs > 0:\n",
    "        obs_nodes = torch.zeros(num_obs, d, device=device)\n",
    "        obs_vals  = torch.zeros(num_obs, d, device=device)\n",
    "        obs_out   = scm.rsample(A, order, interv_node=None, interv_val=None, batch_shape=(num_obs,))\n",
    "    else:\n",
    "        obs_nodes = torch.zeros(0, d, device=device)\n",
    "        obs_vals  = torch.zeros(0, d, device=device)\n",
    "        obs_out   = torch.zeros(0, d, device=device)\n",
    "\n",
    "    # --- Baselines ---\n",
    "    rand_policy = RandomPolicy(d=d, device=device).to(device)\n",
    "    obs_policy  = ObservationPolicy(d=d, device=device).to(device)\n",
    "\n",
    "    # --- Rollouts ---\n",
    "    A_np = A.detach().cpu().numpy()\n",
    "    trained_nodes, trained_vals, trained_outs = run_policy(policy, scm=scm, A=A, order=order, d=d, T=T, device=device,\n",
    "                                                           obs_nodes=obs_nodes, obs_vals=obs_vals, obs_out=obs_out)\n",
    "    random_nodes, random_vals, random_outs = run_policy(rand_policy, scm=scm, A=A, order=order, d=d, T=T, device=device,\n",
    "                                                        obs_nodes=obs_nodes, obs_vals=obs_vals, obs_out=obs_out)\n",
    "    obs_only_nodes, obs_only_vals, obs_only_outs = run_policy(obs_policy, scm=scm, A=A, order=order, d=d, T=T, device=device,\n",
    "                                                              obs_nodes=obs_nodes, obs_vals=obs_vals, obs_out=obs_out)\n",
    "    # --- AVICI metrics ---\n",
    "    g_prob_tr, shd_tr, f1_tr, auroc_tr = _avici_metrics(trained_nodes, trained_outs, A_np, avici_model)\n",
    "    g_prob_rand, shd_rand, f1_rand, auroc_rand = _avici_metrics(random_nodes, random_outs, A_np, avici_model)\n",
    "    g_prob_obs, shd_obs, f1_obs, auroc_obs = _avici_metrics(obs_only_nodes, obs_only_outs, A_np, avici_model)\n",
    "\n",
    "    print(f\"  SHD: {shd_tr},  F1: {f1_tr:.4f},  AUROC: {auroc_tr:.4f},  Interventions: {int(trained_nodes.sum().item())}\")\n",
    "    print(f\"  SHD: {shd_rand},  F1: {f1_rand:.4f},  AUROC: {auroc_rand:.4f},  Interventions: {int(random_nodes.sum().item())}\")\n",
    "    print(f\"  SHD: {shd_obs},  F1: {f1_obs:.4f},  AUROC: {auroc_obs:.4f},  Interventions: {int(obs_only_nodes.sum().item())}\")\n",
    "\n",
    "    plot_adj_heatmap([g_prob_tr, g_prob_rand, g_prob_obs], true=A_np, titles=[\"Trained\", \"Random\", \"Observation\"])\n",
    "    \n",
    "    # --- Visualize & summarize ---\n",
    "    if visualize_interv:\n",
    "        plot_interventions(d, T, A, trained_nodes, trained_vals, scm, policy_title=\"Trained\")\n",
    "        plot_interventions(d, T, A, random_nodes,  random_vals, scm, policy_title=\"Random\")\n",
    "        plot_interventions(d, T, A, obs_only_nodes, obs_only_vals, scm, policy_title=\"Observation\")\n",
    "\n",
    "    return {\n",
    "        \"A\": A, \"order\": order,\n",
    "        \"obs\":  (obs_nodes, obs_vals, obs_out),\n",
    "        \"trained\": {\"nodes\": trained_nodes, \"vals\": trained_vals, \"outs\": trained_outs,\n",
    "                    \"g_prob\": g_prob_tr, \"shd\": shd_tr, \"f1\": f1_tr, \"auroc\": auroc_tr},\n",
    "        \"random\": {\"nodes\": random_nodes, \"vals\": random_vals, \"outs\": random_outs,\n",
    "                   \"g_prob\": g_prob_rand, \"shd\": shd_rand, \"f1\": f1_rand, \"auroc\": auroc_rand},\n",
    "        \"obs_only\": {\"nodes\": obs_only_nodes, \"vals\": obs_only_vals, \"outs\": obs_only_outs,\n",
    "                     \"g_prob\": g_prob_obs, \"shd\": shd_obs, \"f1\": f1_obs, \"auroc\": auroc_obs}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc03a543",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dict = eval_policy(trained_policy, avici_model, d=5, T=7, num_obs=20, visualize_interv=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633cf4c2",
   "metadata": {},
   "source": [
    "Next, we compare the sample efficiency between trained, random, and observational policies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9cff0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for metrics\n",
    "def compute_logp(A_np, g_prob, eps=1e-8):\n",
    "    \"\"\"log posterior probability\"\"\"\n",
    "    p_clipped = np.clip(g_prob, eps, 1.0 - eps)\n",
    "    logp_mat = A_np * np.log(p_clipped) + (1 - A_np) * np.log(1 - p_clipped)\n",
    "    return logp_mat.sum()\n",
    "\n",
    "def compute_caasl(A_np, g_prob):\n",
    "    \"\"\"Expected Number of Correct Entries\"\"\"\n",
    "    g_t_mat = A_np * g_prob + (1 - A_np) * (1 - g_prob)\n",
    "    return g_t_mat.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d86db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "d = 5\n",
    "p = 0.583\n",
    "N = 30\n",
    "n_mc = 200\n",
    "eps = 1e-8\n",
    "num_obs = 20\n",
    "\n",
    "obs_policy = ObservationPolicy(d=d, device=device)\n",
    "rand_policy = RandomPolicy(d=d, device=device)\n",
    "trained_policy = trained_policy.to(device)\n",
    "\n",
    "# Initialize storage\n",
    "shd_obs_all = np.zeros((n_mc, N))\n",
    "shd_rand_all = np.zeros((n_mc, N))\n",
    "shd_trained_all = np.zeros((n_mc, N))\n",
    "auc_obs_all = np.zeros((n_mc, N))\n",
    "auc_rand_all = np.zeros((n_mc, N))\n",
    "auc_trained_all = np.zeros((n_mc, N))\n",
    "logp_obs_all = np.zeros((n_mc, N))\n",
    "logp_rand_all = np.zeros((n_mc, N))\n",
    "logp_trained_all = np.zeros((n_mc, N))\n",
    "caasl_obs_all = np.zeros((n_mc, N))\n",
    "caasl_rand_all = np.zeros((n_mc, N))\n",
    "caasl_trained_all = np.zeros((n_mc, N))\n",
    "\n",
    "for mc in range(n_mc):\n",
    "    # Sample DAG/mechanism\n",
    "    A, order = prior(d=d, p=p, device=device)\n",
    "    A_np = A.detach().cpu().numpy().astype(int)\n",
    "    scm = LinearGaussianSCM(A, device=device)\n",
    "\n",
    "    # Observational prefix\n",
    "    obs_data_nodes = torch.zeros(num_obs, d, device=device)\n",
    "    obs_data_vals = torch.zeros(num_obs, d, device=device)\n",
    "    obs_data_outs = scm.rsample(A, order, batch_shape=(num_obs,))\n",
    "\n",
    "    # Run policies for full horizon N\n",
    "    obs_nodes, obs_vals, obs_outs = run_policy(obs_policy, scm, A, order, d, N, device)\n",
    "    rand_nodes, rand_vals, rand_outs = run_policy(rand_policy, scm, A, order, d, N, device)\n",
    "    trained_nodes, trained_vals, trained_outs = run_policy(trained_policy, scm, A, order, d, N, device,\n",
    "                                                           obs_nodes=obs_data_nodes, obs_vals=obs_data_vals, obs_out=obs_data_outs)\n",
    "    # Evaluate over n = 1,...,N\n",
    "    for n in range(1, N + 1):\n",
    "        # Observational policy (no prefix)\n",
    "        outs_np = obs_outs[:n].detach().cpu().numpy()\n",
    "        nodes_np = obs_nodes[:n].detach().cpu().numpy()\n",
    "\n",
    "        g_prob = avici_model(x=outs_np, interv=nodes_np)\n",
    "        pred_adj = (g_prob > 0.5).astype(int)\n",
    "\n",
    "        shd_obs_all[mc, n-1] = shd(A_np, pred_adj)\n",
    "        auc_obs_all[mc, n-1] = threshold_metrics(A_np, g_prob)['auroc']\n",
    "        logp_obs_all[mc, n-1] = compute_logp(A_np, g_prob, eps)\n",
    "        caasl_obs_all[mc, n-1] = compute_caasl(A_np, g_prob)\n",
    "\n",
    "        # Random policy (no prefix)\n",
    "        outs_np_r = rand_outs[:n].detach().cpu().numpy()\n",
    "        nodes_np_r = rand_nodes[:n].detach().cpu().numpy()\n",
    "\n",
    "        g_prob_r = avici_model(x=outs_np_r, interv=nodes_np_r)\n",
    "        pred_adj_r = (g_prob_r > 0.5).astype(int)\n",
    "\n",
    "        shd_rand_all[mc, n-1] = shd(A_np, pred_adj_r)\n",
    "        auc_rand_all[mc, n-1] = threshold_metrics(A_np, g_prob_r)['auroc']\n",
    "        logp_rand_all[mc, n-1] = compute_logp(A_np, g_prob_r, eps)\n",
    "        caasl_rand_all[mc, n-1] = compute_caasl(A_np, g_prob_r)\n",
    "\n",
    "        # Trained policy (exclude observational prefix from evaluation)\n",
    "        outs_np_t = trained_outs[:n].detach().cpu().numpy()\n",
    "        nodes_np_t = trained_nodes[:n].detach().cpu().numpy()\n",
    "\n",
    "        g_prob_t = avici_model(x=outs_np_t, interv=nodes_np_t)\n",
    "        pred_adj_t = (g_prob_t > 0.5).astype(int)\n",
    "\n",
    "        shd_trained_all[mc, n-1] = shd(A_np, pred_adj_t)\n",
    "        auc_trained_all[mc, n-1] = threshold_metrics(A_np, g_prob_t)['auroc']\n",
    "        logp_trained_all[mc, n-1] = compute_logp(A_np, g_prob_t, eps)\n",
    "        caasl_trained_all[mc, n-1] = compute_caasl(A_np, g_prob_t)\n",
    "\n",
    "# --- mean + SE across MC ---\n",
    "def mean_se(arr):\n",
    "    return arr.mean(axis=0), arr.std(axis=0) / np.sqrt(max(1, arr.shape[0]))\n",
    "\n",
    "mean_shd_obs, se_shd_obs = mean_se(shd_obs_all)\n",
    "mean_shd_rand, se_shd_rand = mean_se(shd_rand_all)\n",
    "mean_shd_trained, se_shd_trained = mean_se(shd_trained_all)\n",
    "mean_auc_obs, se_auc_obs = mean_se(auc_obs_all)\n",
    "mean_auc_rand, se_auc_rand = mean_se(auc_rand_all)\n",
    "mean_auc_trained, se_auc_trained = mean_se(auc_trained_all)\n",
    "mean_logp_obs, se_logp_obs = mean_se(logp_obs_all)\n",
    "mean_logp_rand, se_logp_rand = mean_se(logp_rand_all)\n",
    "mean_logp_trained, se_logp_trained = mean_se(logp_trained_all)\n",
    "mean_caasl_obs, se_caasl_obs = mean_se(caasl_obs_all)\n",
    "mean_caasl_rand, se_caasl_rand = mean_se(caasl_rand_all)\n",
    "mean_caasl_trained, se_caasl_trained = mean_se(caasl_trained_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b4f862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plot results ---\n",
    "\n",
    "ns = np.arange(1, N + 1)\n",
    "metrics = [\n",
    "    (\"SHD\", mean_shd_obs, se_shd_obs, mean_shd_rand, se_shd_rand, mean_shd_trained, se_shd_trained),\n",
    "    (\"AUROC\", mean_auc_obs, se_auc_obs, mean_auc_rand, se_auc_rand, mean_auc_trained, se_auc_trained),\n",
    "    (\"Log Posterior Probability\", mean_logp_obs, se_logp_obs, mean_logp_rand, se_logp_rand, mean_logp_trained, se_logp_trained),\n",
    "    (\"Expected Number of Correct Entries\", mean_caasl_obs, se_caasl_obs, mean_caasl_rand, se_caasl_rand, mean_caasl_trained, se_caasl_trained)]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, (ax, (title, mean_o, se_o, mean_r, se_r, mean_t, se_t)) in enumerate(zip(axes, metrics)):\n",
    "    ax.plot(ns, mean_o, label=\"Observation\", color=\"black\")\n",
    "    ax.fill_between(ns, mean_o - se_o, mean_o + se_o, color=\"black\", alpha=0.2)\n",
    "    ax.plot(ns, mean_r, label=\"Random\", color=\"blue\")\n",
    "    ax.fill_between(ns, mean_r - se_r, mean_r + se_r, color=\"blue\", alpha=0.2)\n",
    "    ax.plot(ns, mean_t, label=\"Trained\", color=\"red\")\n",
    "    ax.fill_between(ns, mean_t - se_t, mean_t + se_t, color=\"red\", alpha=0.2)\n",
    "    ax.set_xlabel(\"Total Time Periods (T)\", fontsize=12)\n",
    "    ax.set_ylabel(None)\n",
    "    ax.set_title(title)\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    \n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "\n",
    "plt.suptitle(rf\"Graph Identifiability Across Sample Sizes ($d={d}$)\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3281e1c",
   "metadata": {},
   "source": [
    "### 8. Effects of Entropy Target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1423f79",
   "metadata": {},
   "source": [
    "First, we define a helper function `eval_policy_params()` that will allow us to run a Monte Carlo evaluation of a list of trained policies under given environmental parameters `w`, `sigma`, and `p`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3290116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_policy_params(\n",
    "    d: int,\n",
    "    p: float,\n",
    "    w: float | None = None,\n",
    "    sigma: float | None = None,\n",
    "    n_mc: int = 50,\n",
    "    T: int = 8,\n",
    "    num_obs: int = 20,\n",
    "    trained_policies: list = None,\n",
    "    avici_model=None,\n",
    "    device: torch.device = torch.device('cpu'),\n",
    "    action_mode: str = \"sample\",  # \"sample\" or \"mean\"\n",
    "    eps: float = 1e-8):\n",
    "\n",
    "    # Initialize\n",
    "    obs_policy = ObservationPolicy(d=d, device=device)\n",
    "    rand_policy = RandomPolicy(d=d, device=device)\n",
    "    trained_policies = [policy.to(device) for policy in trained_policies]\n",
    "\n",
    "    metrics_names = ['shd', 'auroc', 'logp', 'caasl']\n",
    "    baseline_policies = ['obs', 'rand']\n",
    "    trained_policy_names = [f'trained_{i}' for i in range(len(trained_policies))]\n",
    "    all_policy_names = baseline_policies + trained_policy_names\n",
    "\n",
    "    results = {}\n",
    "    for policy in all_policy_names:\n",
    "        results[policy] = {}\n",
    "        for metric in metrics_names:\n",
    "            results[policy][metric] = np.zeros(n_mc)  \n",
    "\n",
    "    for mc in range(n_mc):\n",
    "        if mc % 20 == 0:\n",
    "            print(f\"mc: {mc}\")\n",
    "            \n",
    "        # Sample DAG\n",
    "        A, order = prior(d=d, p=p, device=device)\n",
    "        A_np = A.detach().cpu().numpy().astype(int)\n",
    "\n",
    "        # Create SCM with specified parameters\n",
    "        W_base = None\n",
    "        if w is not None:\n",
    "            # Use custom edge weight scaling\n",
    "            in_degree = A.sum(dim=-2, keepdim=True).clamp_min(1.0)\n",
    "            coeff_std = (w / torch.sqrt(in_degree)).expand_as(A)\n",
    "            W_base = torch.normal(mean=0.0, std=coeff_std)\n",
    "\n",
    "        sigma_vec = None\n",
    "        if sigma is not None:\n",
    "            sigma_vec = torch.full((d,), sigma, device=device)\n",
    "        \n",
    "        scm = LinearGaussianSCM(A, W_base=W_base, sigma=sigma_vec, device=device)\n",
    "\n",
    "        # Generate observational prefix\n",
    "        obs_data_nodes = torch.zeros(num_obs, d, device=device)\n",
    "        obs_data_vals = torch.zeros(num_obs, d, device=device)\n",
    "        obs_data_outs = scm.rsample(A, order, batch_shape=(num_obs,))\n",
    "\n",
    "        # Run baseline policies\n",
    "        obs_nodes, obs_vals, obs_outs = run_policy(obs_policy, scm, A, order, d, T, device)\n",
    "        rand_nodes, rand_vals, rand_outs = run_policy(rand_policy, scm, A, order, d, T, device)\n",
    "\n",
    "        # Run all trained policies (using specified action_mode)\n",
    "        trained_results = []\n",
    "        for trained_policy in trained_policies:\n",
    "            trained_nodes, trained_vals, trained_outs = run_policy(trained_policy, scm, A, order, d, T, device,\n",
    "                                                                    obs_nodes=obs_data_nodes, obs_vals=obs_data_vals, obs_out=obs_data_outs,\n",
    "                                                                    action_mode=action_mode)\n",
    "            trained_results.append((trained_nodes, trained_vals, trained_outs))\n",
    "\n",
    "        # Save the policy trajectories\n",
    "        policy_data = {'obs': (obs_outs, obs_nodes),  'rand': (rand_outs, rand_nodes)}\n",
    "        for i, (trained_nodes, trained_vals, trained_outs) in enumerate(trained_results):\n",
    "            policy_data[f'trained_{i}'] = (trained_outs, trained_nodes)\n",
    "\n",
    "        for policy_name, (outs, nodes) in policy_data.items():\n",
    "            outs_np = outs.detach().cpu().numpy()\n",
    "            nodes_np = nodes.detach().cpu().numpy()\n",
    "\n",
    "            # Calculate performance metrics\n",
    "            g_prob = avici_model(x=outs_np, interv=nodes_np)\n",
    "            pred_adj = (g_prob > 0.5).astype(int)\n",
    "\n",
    "            results[policy_name]['shd'][mc] = shd(A_np, pred_adj)\n",
    "            results[policy_name]['auroc'][mc] = threshold_metrics(A_np, g_prob)['auroc']\n",
    "            results[policy_name]['logp'][mc] = compute_logp(A_np, g_prob, eps)\n",
    "            results[policy_name]['caasl'][mc] = compute_caasl(A_np, g_prob)\n",
    "\n",
    "    def mean_se(arr):\n",
    "        return arr.mean(), arr.std() / np.sqrt(max(1, len(arr)))\n",
    "\n",
    "    # Store results\n",
    "    stats = {}\n",
    "    for policy_name in all_policy_names:\n",
    "        stats[policy_name] = {}\n",
    "        for metric in metrics_names:\n",
    "            mean_val, se_val = mean_se(results[policy_name][metric])\n",
    "            stats[policy_name][metric] = {'mean': mean_val,\n",
    "                                          'se': se_val,\n",
    "                                          'raw': results[policy_name][metric]}\n",
    "\n",
    "    return {'stats': stats, 'policy_names': all_policy_names}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ec5d20",
   "metadata": {},
   "source": [
    "The target entropy experiment is broken down into 3 stages: training, evaluation, and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd2cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---------- Training Stage ---------- #\n",
    "\n",
    "entropy_pairs = [\n",
    "    (0.01, -2.0),     \n",
    "    (0.25, -1.0),      \n",
    "    (0.75, 0.0),     \n",
    "    (0.98, 0.68)]\n",
    "\n",
    "trained_models = {}\n",
    "\n",
    "for i, (target_entropy_disc, target_entropy_cont) in enumerate(entropy_pairs):\n",
    "    print(f\"\\nTraining Policy {i+1}/{len(entropy_pairs)}\")\n",
    "    print(f\"Target Entropy: Discrete {target_entropy_disc:.3f}, Continuous {target_entropy_cont:.3f}\")\n",
    "\n",
    "    # Re-train the policy for each entropy pair\n",
    "    training_stats, sac, replay_buffer, trained_policy = train(\n",
    "        d=5,\n",
    "        T=8,\n",
    "        p=0.583,\n",
    "        batch_size=128,\n",
    "        num_episodes=4000, \n",
    "        num_obs=20,\n",
    "        print_every=100,  \n",
    "        device=torch.device(\"cuda\"),\n",
    "        avici_model=avici_model,\n",
    "        tune_entropy=True,\n",
    "        target_entropy_disc=target_entropy_disc * math.log(5),\n",
    "        target_entropy_cont=target_entropy_cont,\n",
    "        alpha_d=0.5,\n",
    "        alpha_c=0.5,\n",
    "        updates_per_step=1,\n",
    "        warmup_episodes=500)\n",
    "\n",
    "    # Store results\n",
    "    key = f\"model_{i}_disc{target_entropy_disc:.2f}_cont{target_entropy_cont:.2f}\"\n",
    "    trained_models[key] = {\n",
    "        'training_stats': training_stats,\n",
    "        'sac': sac,\n",
    "        'replay_buffer': replay_buffer,\n",
    "        'trained_policy': trained_policy,\n",
    "        'target_entropy_disc': target_entropy_disc,\n",
    "        'target_entropy_cont': target_entropy_cont,\n",
    "        'model_index': i}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a072d50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Evaluation Stage ---------- #\n",
    "\n",
    "n_mc = 300\n",
    "\n",
    "# Extract the trained policies\n",
    "trained_policies_list = [trained_models[key]['trained_policy'] for key in trained_models.keys()]\n",
    "\n",
    "# Run evaluation with all 4 trained policies using the SAMPLE action mode\n",
    "\n",
    "results_sample = eval_policy_params(\n",
    "    d=5,\n",
    "    p=0.583,\n",
    "    w=None,\n",
    "    sigma=None,\n",
    "    n_mc=n_mc,  # Increase for more stable results\n",
    "    T=8,\n",
    "    num_obs=20,\n",
    "    trained_policies=trained_policies_list,\n",
    "    avici_model=avici_model,\n",
    "    device=device,\n",
    "    action_mode=\"sample\")\n",
    "\n",
    "# Run evaluation with all 4 trained policies using the MEAN action mode\n",
    "\n",
    "results_mean = eval_policy_params(\n",
    "    d=5,\n",
    "    p=0.583,\n",
    "    w=None,\n",
    "    sigma=None,\n",
    "    n_mc=n_mc,  \n",
    "    T=8,\n",
    "    num_obs=20,\n",
    "    trained_policies=trained_policies_list,\n",
    "    avici_model=avici_model,\n",
    "    device=device,\n",
    "    action_mode=\"mean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b556d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Plotting Stage ---------- #\n",
    "\n",
    "metrics = ['shd', 'auroc', 'logp', 'caasl']\n",
    "\n",
    "# Prepare data for plotting\n",
    "scaled_pairs = [(x * math.log(5), y) for x, y in entropy_pairs]\n",
    "entropy_labels = [f\"({pair[0]:.2f}, {pair[1]:.2f})\" for pair in scaled_pairs]\n",
    "x_positions = np.arange(len(entropy_pairs))\n",
    "\n",
    "# Extract baseline policy results\n",
    "obs_results = {}\n",
    "rand_results = {}\n",
    "trained_sample_results = {metric: {'means': [], 'ses': []} for metric in metrics}\n",
    "trained_mean_results = {metric: {'means': [], 'ses': []} for metric in metrics}\n",
    "\n",
    "for metric in metrics:\n",
    "    # Baseline policies\n",
    "    obs_mean = results_sample['stats']['obs'][metric]['mean']\n",
    "    obs_se = results_sample['stats']['obs'][metric]['se']\n",
    "    obs_results[metric] = {'mean': obs_mean, 'se': obs_se}\n",
    "\n",
    "    rand_mean = results_sample['stats']['rand'][metric]['mean']\n",
    "    rand_se = results_sample['stats']['rand'][metric]['se']\n",
    "    rand_results[metric] = {'mean': rand_mean, 'se': rand_se}\n",
    "\n",
    "    # Trained policies (SAMPLE mode)\n",
    "    for i in range(len(entropy_pairs)):\n",
    "        policy_name = f'trained_{i}'\n",
    "        mean_val = results_sample['stats'][policy_name][metric]['mean']\n",
    "        se_val = results_sample['stats'][policy_name][metric]['se']\n",
    "        trained_sample_results[metric]['means'].append(mean_val)\n",
    "        trained_sample_results[metric]['ses'].append(se_val)\n",
    "\n",
    "    # Trained policies (MEAN mode)\n",
    "    for i in range(len(entropy_pairs)):\n",
    "        policy_name = f'trained_{i}'\n",
    "        mean_val = results_mean['stats'][policy_name][metric]['mean']\n",
    "        se_val = results_mean['stats'][policy_name][metric]['se']\n",
    "        trained_mean_results[metric]['means'].append(mean_val)\n",
    "        trained_mean_results[metric]['ses'].append(se_val)\n",
    "\n",
    "    # Convert to numpy\n",
    "    trained_sample_results[metric]['means'] = np.array(trained_sample_results[metric]['means'])\n",
    "    trained_sample_results[metric]['ses'] = np.array(trained_sample_results[metric]['ses'])\n",
    "    trained_mean_results[metric]['means'] = np.array(trained_mean_results[metric]['means'])\n",
    "    trained_mean_results[metric]['ses'] = np.array(trained_mean_results[metric]['ses'])\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "metric_titles = {\n",
    "    'shd': 'SHD',\n",
    "    'auroc': 'AUROC',\n",
    "    'logp': 'Log Posterior Probability',\n",
    "    'caasl': 'Expected Number of Correct Entries'}\n",
    "\n",
    "for i, (ax, metric) in enumerate(zip(axes, metrics)):\n",
    "\n",
    "    # Observational policy\n",
    "    obs_mean = obs_results[metric]['mean']\n",
    "    obs_se = obs_results[metric]['se']\n",
    "    ax.plot(x_positions, [obs_mean] * len(x_positions),\n",
    "            label='Observational', color=\"black\", linewidth=2)\n",
    "    ax.fill_between(x_positions,\n",
    "                    [obs_mean - obs_se] * len(x_positions),\n",
    "                    [obs_mean + obs_se] * len(x_positions),\n",
    "                    color=\"black\", alpha=0.2)\n",
    "\n",
    "    # Random policy\n",
    "    rand_mean = rand_results[metric]['mean']\n",
    "    rand_se = rand_results[metric]['se']\n",
    "    ax.plot(x_positions, [rand_mean] * len(x_positions),\n",
    "            label='Random', color=\"blue\", linewidth=2)\n",
    "    ax.fill_between(x_positions,\n",
    "                    [rand_mean - rand_se] * len(x_positions),\n",
    "                    [rand_mean + rand_se] * len(x_positions),\n",
    "                    color=\"blue\", alpha=0.2)\n",
    "\n",
    "    # Trained policies (SAMPLE mode)\n",
    "    trained_sample_means = trained_sample_results[metric]['means']\n",
    "    trained_sample_ses = trained_sample_results[metric]['ses']\n",
    "    ax.plot(x_positions, trained_sample_means,\n",
    "            label='Trained (Sample)', color=\"purple\", linewidth=2, marker='o')\n",
    "    ax.fill_between(x_positions,\n",
    "                    trained_sample_means - trained_sample_ses,\n",
    "                    trained_sample_means + trained_sample_ses,\n",
    "                    color=\"purple\", alpha=0.2)\n",
    "\n",
    "    # Trained policies (MEAN mode)\n",
    "    trained_mean_means = trained_mean_results[metric]['means']\n",
    "    trained_mean_ses = trained_mean_results[metric]['ses']\n",
    "    ax.plot(x_positions, trained_mean_means,\n",
    "            label='Trained (Mean)', color=\"red\", linewidth=2, marker='s')\n",
    "    ax.fill_between(x_positions,\n",
    "                    trained_mean_means - trained_mean_ses,\n",
    "                    trained_mean_means + trained_mean_ses,\n",
    "                    color=\"red\", alpha=0.2)\n",
    "\n",
    "    # Formatting\n",
    "    ax.set_xlabel('Target Entropy (Discrete, Continuous)', fontsize=12, labelpad=10)\n",
    "    ax.set_title(metric_titles[metric], fontsize=14)\n",
    "    ax.set_xticks(x_positions)\n",
    "    ax.set_xticklabels(entropy_labels, fontsize=12, ha='center')\n",
    "    ax.tick_params(axis='y', labelsize=12)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "    if i == 0:\n",
    "        ax.legend(loc=\"upper center\", bbox_to_anchor=(0.19, 0.8))\n",
    "\n",
    "plt.suptitle(f'Policy Performance vs. Target Entropy', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "089d01fe",
   "metadata": {},
   "source": [
    "### 9. Simulator Shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec2eb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the list of trained models\n",
    "trained_models.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf225151",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_values = np.linspace(0.1, 0.9, 5)\n",
    "\n",
    "# Define baseline p value\n",
    "d = 5\n",
    "baseline_p = 0.583\n",
    "n_mc = 300\n",
    "\n",
    "# Assume you have two trained policies\n",
    "trained_policy1 = trained_models[\"model_0_disc0.01_cont-2.00\"][\"trained_policy\"]\n",
    "trained_policy2 = trained_models[\"model_2_disc0.75_cont0.00\"][\"trained_policy\"]\n",
    "\n",
    "# We'll evaluate each policy separately with its desired action mode\n",
    "actual_policies = ['obs', 'rand', 'trained_0_mean', 'trained_1_sample']\n",
    "\n",
    "# Storage for results across all p values\n",
    "all_results = {}\n",
    "metrics = ['shd', 'auroc', 'logp', 'caasl']\n",
    "\n",
    "# Initialize storage for all actual policies\n",
    "for policy in actual_policies:\n",
    "    all_results[policy] = {}\n",
    "    for metric in metrics:\n",
    "        all_results[policy][metric] = {'means': [], 'ses': []}\n",
    "\n",
    "# Loop over p values\n",
    "for i, p in enumerate(p_values):\n",
    "    print(f\"Evaluating p = {p:.3f} ({i+1}/{len(p_values)})\")\n",
    "\n",
    "    # Run evaluation for trained_policy1 with action_mode=\"mean\"\n",
    "    results_mean = eval_policy_params(d=d, p=p, w=None, sigma=None, n_mc=n_mc, T=8, num_obs=20,\n",
    "                                      trained_policies=[trained_policy1],\n",
    "                                      avici_model=avici_model, device=device, action_mode=\"mean\")\n",
    "\n",
    "    # Run evaluation for trained_policy2 with action_mode=\"sample\"\n",
    "    results_sample = eval_policy_params(d=d, p=p, w=None, sigma=None, n_mc=n_mc, T=8, num_obs=20,\n",
    "                                        trained_policies=[trained_policy2], \n",
    "                                        avici_model=avici_model, device=device, action_mode=\"sample\")\n",
    "\n",
    "    # Extract results\n",
    "    for metric in metrics:\n",
    "        # Baseline policies (same in both results)\n",
    "        all_results['obs'][metric]['means'].append(results_mean['stats']['obs'][metric]['mean'])\n",
    "        all_results['obs'][metric]['ses'].append(results_mean['stats']['obs'][metric]['se'])\n",
    "\n",
    "        all_results['rand'][metric]['means'].append(results_mean['stats']['rand'][metric]['mean'])\n",
    "        all_results['rand'][metric]['ses'].append(results_mean['stats']['rand'][metric]['se'])\n",
    "\n",
    "        # Trained policy 1 (mean mode)\n",
    "        all_results['trained_0_mean'][metric]['means'].append(results_mean['stats']['trained_0'][metric]['mean'])\n",
    "        all_results['trained_0_mean'][metric]['ses'].append(results_mean['stats']['trained_0'][metric]['se'])\n",
    "\n",
    "        # Trained policy 2 (sample mode)\n",
    "        all_results['trained_1_sample'][metric]['means'].append(results_sample['stats']['trained_0'][metric]['mean'])\n",
    "        all_results['trained_1_sample'][metric]['ses'].append(results_sample['stats']['trained_0'][metric]['se'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad63f8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy \n",
    "for policy in actual_policies:\n",
    "    for metric in metrics:\n",
    "        all_results[policy][metric]['means'] = np.array(all_results[policy][metric]['means'])\n",
    "        all_results[policy][metric]['ses'] = np.array(all_results[policy][metric]['ses'])\n",
    "\n",
    "selected_metrics = ['shd', 'auroc', 'caasl']\n",
    "\n",
    "# Plotting\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "colors = {\n",
    "    'obs': 'black',\n",
    "    'rand': 'blue',\n",
    "    'trained_0_mean': 'red',\n",
    "    'trained_1_sample': 'purple'\n",
    "}\n",
    "\n",
    "labels = {\n",
    "    'obs': 'Observational',\n",
    "    'rand': 'Random',\n",
    "    'trained_0_mean': 'Deterministic',\n",
    "    'trained_1_sample': 'Stochastic'\n",
    "}\n",
    "\n",
    "metric_titles = {\n",
    "    'shd': 'SHD',\n",
    "    'auroc': 'AUROC',\n",
    "    'logp': 'Log Posterior Probability',\n",
    "    'caasl': 'Expected Number of Correct Entries'\n",
    "}\n",
    "\n",
    "for i, (ax, metric) in enumerate(zip(axes, selected_metrics)):\n",
    "    ax.axvline(x=baseline_p, color='gray', linestyle='--', alpha=0.7, linewidth=2,\n",
    "                label=f'Baseline')\n",
    "\n",
    "    for policy in actual_policies:\n",
    "        if policy == 'obs':\n",
    "            continue\n",
    "\n",
    "        means = all_results[policy][metric]['means']\n",
    "        ses = all_results[policy][metric]['ses']\n",
    "\n",
    "        ax.plot(p_values, means, label=labels[policy], color=colors[policy], linewidth=2)\n",
    "        ax.fill_between(p_values, means - ses, means + ses, color=colors[policy], alpha=0.2)\n",
    "\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "\n",
    "    ax.set_xlabel('Edge Probability (p)', fontsize=12)\n",
    "    ax.set_title(metric_titles[metric], fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.tick_params(axis='both', labelsize=12)\n",
    "\n",
    "plt.suptitle(f'Policy Performance vs. Edge Probability (d={d})', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f475062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sigma Shift --- #\n",
    "\n",
    "sigma_values = np.linspace(0.001, 1.0, 5)\n",
    "d = 5\n",
    "baseline_sigma = 0.11\n",
    "n_mc = 200\n",
    "\n",
    "trained_policy1 = trained_models[\"model_0_disc0.01_cont-2.00\"][\"trained_policy\"]  # for mean\n",
    "trained_policy2 = trained_models[\"model_2_disc0.75_cont0.00\"][\"trained_policy\"]  # for sample\n",
    "\n",
    "all_results = {}\n",
    "metrics = ['shd', 'auroc', 'logp', 'caasl']\n",
    "\n",
    "policy_names = ['obs', 'rand', 'trained_sample', 'trained_mean']\n",
    "\n",
    "# Initialize storage\n",
    "for policy in policy_names:\n",
    "    all_results[policy] = {}\n",
    "    for metric in metrics:\n",
    "        all_results[policy][metric] = {'means': [], 'ses': []}\n",
    "\n",
    "# Loop over sigma values\n",
    "for i, sigma in enumerate(sigma_values):\n",
    "\n",
    "    # Run evaluation with trained_policy1 in MEAN mode\n",
    "    results_mean = eval_policy_params(\n",
    "        d=d,\n",
    "        p=0.583,\n",
    "        w=None,\n",
    "        sigma=sigma,\n",
    "        n_mc=n_mc,\n",
    "        T=8,\n",
    "        num_obs=20,\n",
    "        trained_policies=[trained_policy1],\n",
    "        avici_model=avici_model,\n",
    "        device=device,\n",
    "        action_mode=\"mean\")\n",
    "    \n",
    "    # Run evaluation with trained_policy2 in SAMPLE mode\n",
    "    results_sample = eval_policy_params(\n",
    "        d=d,\n",
    "        p=0.583,\n",
    "        w=None,\n",
    "        sigma=sigma,\n",
    "        n_mc=n_mc,\n",
    "        T=8,\n",
    "        num_obs=20,\n",
    "        trained_policies=[trained_policy2],\n",
    "        avici_model=avici_model,\n",
    "        device=device,\n",
    "        action_mode=\"sample\")\n",
    "\n",
    "    for metric in metrics:\n",
    "        # Baseline policies\n",
    "        obs_mean = results_sample['stats']['obs'][metric]['mean']\n",
    "        obs_se = results_sample['stats']['obs'][metric]['se']\n",
    "        all_results['obs'][metric]['means'].append(obs_mean)\n",
    "        all_results['obs'][metric]['ses'].append(obs_se)\n",
    "\n",
    "        rand_mean = results_sample['stats']['rand'][metric]['mean']\n",
    "        rand_se = results_sample['stats']['rand'][metric]['se']\n",
    "        all_results['rand'][metric]['means'].append(rand_mean)\n",
    "        all_results['rand'][metric]['ses'].append(rand_se)\n",
    "\n",
    "        # Trained policy SAMPLE mode \n",
    "        sample_mean = results_sample['stats']['trained_0'][metric]['mean']\n",
    "        sample_se = results_sample['stats']['trained_0'][metric]['se']\n",
    "        all_results['trained_sample'][metric]['means'].append(sample_mean)\n",
    "        all_results['trained_sample'][metric]['ses'].append(sample_se)\n",
    "\n",
    "        # Trained policy MEAN mode \n",
    "        mean_mean = results_mean['stats']['trained_0'][metric]['mean']\n",
    "        mean_se = results_mean['stats']['trained_0'][metric]['se']\n",
    "        all_results['trained_mean'][metric]['means'].append(mean_mean)\n",
    "        all_results['trained_mean'][metric]['ses'].append(mean_se)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0eaa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy\n",
    "for policy in policy_names:\n",
    "    for metric in metrics:\n",
    "        all_results[policy][metric]['means'] = np.array(all_results[policy][metric]['means'])\n",
    "        all_results[policy][metric]['ses'] = np.array(all_results[policy][metric]['ses'])\n",
    "\n",
    "selected_metrics = ['shd', 'auroc', 'caasl']\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "colors = {\n",
    "    'obs': 'black',\n",
    "    'rand': 'blue',\n",
    "    'trained_sample': 'purple',\n",
    "    'trained_mean': 'red'\n",
    "}\n",
    "\n",
    "labels = {\n",
    "    'obs': 'Observational',\n",
    "    'rand': 'Random',\n",
    "    'trained_sample': 'Trained (Sample)',\n",
    "    'trained_mean': 'Trained (Mean)'\n",
    "}\n",
    "\n",
    "metric_titles = {\n",
    "    'shd': 'SHD',\n",
    "    'auroc': 'AUROC',\n",
    "    'caasl': 'Expected Number of Correct Entries'\n",
    "}\n",
    "\n",
    "for i, (ax, metric) in enumerate(zip(axes, selected_metrics)):\n",
    "    ax.axvline(x=baseline_sigma, color='gray', linestyle='--', alpha=0.7, linewidth=2,\n",
    "                label=f'Baseline (Mean)')\n",
    "\n",
    "    for policy in policy_names:\n",
    "        if policy == 'obs':  \n",
    "            continue\n",
    "\n",
    "        means = all_results[policy][metric]['means']\n",
    "        ses = all_results[policy][metric]['ses']\n",
    "\n",
    "        ax.plot(sigma_values, means, label=labels[policy], color=colors[policy], linewidth=2)\n",
    "        ax.fill_between(sigma_values, means - ses, means + ses, color=colors[policy], alpha=0.2)\n",
    "\n",
    "    ax.set_xlabel('Noise Standard Deviation (σ)', fontsize=12)\n",
    "    ax.set_title(metric_titles[metric], fontsize=14)\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.tick_params(axis='both', labelsize=12)\n",
    "\n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "\n",
    "plt.suptitle(f'Policy Performance vs. Noise Standard Deviation', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avici-env2-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
